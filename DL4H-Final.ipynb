{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e1c35a",
   "metadata": {},
   "source": [
    "# Paper Survey - A disease inference method based on symptom extraction and bidirectional Long Short Term Memory networks\n",
    "There are three steps involved in the project:\n",
    "1. **Symptom extraction**: The primary task is the preprocessing of electronic medical texts and the identification of symptom entities. In this part, after filtering out the no-symptom entities from the medical texts according to\n",
    "the structural characteristics of MIMIC-III, we use the existing natural\n",
    "language processing tool MetaMap to identify symptom entities\n",
    "which are extracted from full clinical texts.\n",
    "2. **Symptom representation**: Then the vector representation\n",
    "of symptoms is obtained based on two representations, in which the TF-IDF obtains the strength of the association of each symptom with all the diseases and uses this as an element of the symptom vector. At the same time, the preprocessed text is used to train Word2Vec to obtain the word vector which is utilized to generate the symptom vector.\n",
    "3. **BiLSTMs**: The third part is a multi-label classifier. In this part,\n",
    "we use the symptom sequences to train the multi-label classification\n",
    "model BiLSTMs with two symptom vector representations. The models\n",
    "with different symptom representations are trained separately. Finally,\n",
    "we take the weighted sum of the outputs of BiLSTMs with different\n",
    "symptom representations for predicting candidate diseases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86a109",
   "metadata": {},
   "source": [
    "## Install, Import and Configure required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d328cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import *\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import numpy_indexed as npi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984868b6",
   "metadata": {},
   "source": [
    "# STEP 0: Import the MIMIC-III data\n",
    "\n",
    "In this step we will import the NOTEEVENTS.csv.gz and filter on only \"discharge summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a7d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_data = pd.read_csv(\"NOTEEVENTS.csv\",  dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b325979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>22532</td>\n",
       "      <td>167853</td>\n",
       "      <td>2151-08-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>13702</td>\n",
       "      <td>107527</td>\n",
       "      <td>2118-06-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177</td>\n",
       "      <td>13702</td>\n",
       "      <td>196489</td>\n",
       "      <td>2124-08-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2124-7-21**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>26880</td>\n",
       "      <td>135453</td>\n",
       "      <td>2162-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2162-3-3**]              D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ROW_ID SUBJECT_ID HADM_ID   CHARTDATE CHARTTIME STORETIME  \\\n",
       "0    174      22532  167853  2151-08-04       NaN       NaN   \n",
       "1    175      13702  107527  2118-06-14       NaN       NaN   \n",
       "2    176      13702  167118  2119-05-25       NaN       NaN   \n",
       "3    177      13702  196489  2124-08-18       NaN       NaN   \n",
       "4    178      26880  135453  2162-03-25       NaN       NaN   \n",
       "\n",
       "            CATEGORY DESCRIPTION CGID ISERROR  \\\n",
       "0  Discharge summary      Report  NaN     NaN   \n",
       "1  Discharge summary      Report  NaN     NaN   \n",
       "2  Discharge summary      Report  NaN     NaN   \n",
       "3  Discharge summary      Report  NaN     NaN   \n",
       "4  Discharge summary      Report  NaN     NaN   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
       "1  Admission Date:  [**2118-6-2**]       Discharg...  \n",
       "2  Admission Date:  [**2119-5-4**]              D...  \n",
       "3  Admission Date:  [**2124-7-21**]              ...  \n",
       "4  Admission Date:  [**2162-3-3**]              D...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a832c5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2083180, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc2cda",
   "metadata": {},
   "source": [
    "#### We will filter on Discharge summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54bd0f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_discharge_summary = mimic_data[(mimic_data.CATEGORY == \"Discharge summary\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff2f5ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>22532</td>\n",
       "      <td>167853</td>\n",
       "      <td>2151-08-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>13702</td>\n",
       "      <td>107527</td>\n",
       "      <td>2118-06-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177</td>\n",
       "      <td>13702</td>\n",
       "      <td>196489</td>\n",
       "      <td>2124-08-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2124-7-21**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>26880</td>\n",
       "      <td>135453</td>\n",
       "      <td>2162-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2162-3-3**]              D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ROW_ID SUBJECT_ID HADM_ID   CHARTDATE CHARTTIME STORETIME  \\\n",
       "0    174      22532  167853  2151-08-04       NaN       NaN   \n",
       "1    175      13702  107527  2118-06-14       NaN       NaN   \n",
       "2    176      13702  167118  2119-05-25       NaN       NaN   \n",
       "3    177      13702  196489  2124-08-18       NaN       NaN   \n",
       "4    178      26880  135453  2162-03-25       NaN       NaN   \n",
       "\n",
       "            CATEGORY DESCRIPTION CGID ISERROR  \\\n",
       "0  Discharge summary      Report  NaN     NaN   \n",
       "1  Discharge summary      Report  NaN     NaN   \n",
       "2  Discharge summary      Report  NaN     NaN   \n",
       "3  Discharge summary      Report  NaN     NaN   \n",
       "4  Discharge summary      Report  NaN     NaN   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
       "1  Admission Date:  [**2118-6-2**]       Discharg...  \n",
       "2  Admission Date:  [**2119-5-4**]              D...  \n",
       "3  Admission Date:  [**2124-7-21**]              ...  \n",
       "4  Admission Date:  [**2162-3-3**]              D...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_discharge_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c288e80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59652, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_discharge_summary.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0432abd9",
   "metadata": {},
   "source": [
    "# **STEP 1: Symptom Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94b5dc9",
   "metadata": {},
   "source": [
    "#### CHARTTIME, STORETIME, CGID and ISERROR column has all null, drop these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdf9b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_discharge_summary = mimic_discharge_summary.dropna(axis = 1, how = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a1a8705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3151</th>\n",
       "      <td>2832</td>\n",
       "      <td>18689</td>\n",
       "      <td>117162</td>\n",
       "      <td>2115-06-01</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2115-5-30**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50908</th>\n",
       "      <td>55416</td>\n",
       "      <td>18689</td>\n",
       "      <td>117162</td>\n",
       "      <td>2115-06-02</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Addendum</td>\n",
       "      <td>Name:  [**Known lastname 2132**], [**Known fir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ROW_ID SUBJECT_ID HADM_ID   CHARTDATE           CATEGORY DESCRIPTION  \\\n",
       "3151    2832      18689  117162  2115-06-01  Discharge summary      Report   \n",
       "50908  55416      18689  117162  2115-06-02  Discharge summary    Addendum   \n",
       "\n",
       "                                                    TEXT  \n",
       "3151   Admission Date:  [**2115-5-30**]       Dischar...  \n",
       "50908  Name:  [**Known lastname 2132**], [**Known fir...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_discharge_summary[mimic_discharge_summary.HADM_ID == \"117162\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a35088a",
   "metadata": {},
   "source": [
    "#### We have few rows for Addendum to the original discharge summary, we have to merge these Addendum to the original discharge summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "294b8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_text(group):\n",
    "    main_rows = group[\"DESCRIPTION\"] == \"Report\"\n",
    "    if main_rows.any():\n",
    "        main_text = group.loc[group[\"DESCRIPTION\"] == \"Report\", \"TEXT\"].values[0]\n",
    "        append_texts = group.loc[group[\"DESCRIPTION\"] == \"Addendum\", \"TEXT\"].tolist()\n",
    "        concatenated_text = main_text + \" \" + \" \".join(append_texts)\n",
    "        group.loc[group[\"DESCRIPTION\"] == \"Report\", \"TEXT\"] = concatenated_text\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cbfa4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_mimic_discharge_summary = mimic_discharge_summary.groupby(\"HADM_ID\", group_keys=False).apply(concat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a3ce66a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174</td>\n",
       "      <td>22532</td>\n",
       "      <td>167853</td>\n",
       "      <td>2151-08-04</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>13702</td>\n",
       "      <td>107527</td>\n",
       "      <td>2118-06-14</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177</td>\n",
       "      <td>13702</td>\n",
       "      <td>196489</td>\n",
       "      <td>2124-08-18</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2124-7-21**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>26880</td>\n",
       "      <td>135453</td>\n",
       "      <td>2162-03-25</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2162-3-3**]              D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ROW_ID SUBJECT_ID HADM_ID   CHARTDATE           CATEGORY DESCRIPTION  \\\n",
       "0    174      22532  167853  2151-08-04  Discharge summary      Report   \n",
       "1    175      13702  107527  2118-06-14  Discharge summary      Report   \n",
       "2    176      13702  167118  2119-05-25  Discharge summary      Report   \n",
       "3    177      13702  196489  2124-08-18  Discharge summary      Report   \n",
       "4    178      26880  135453  2162-03-25  Discharge summary      Report   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
       "1  Admission Date:  [**2118-6-2**]       Discharg...  \n",
       "2  Admission Date:  [**2119-5-4**]              D...  \n",
       "3  Admission Date:  [**2124-7-21**]              ...  \n",
       "4  Admission Date:  [**2162-3-3**]              D...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_mimic_discharge_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0133e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_mimic_discharge_summary.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f647166",
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge_summary = grouped_mimic_discharge_summary[grouped_mimic_discharge_summary[\"DESCRIPTION\"] != \"Addendum\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dfdca1",
   "metadata": {},
   "source": [
    "#### According to authors in paper, we will drop rows which does not have Social History, Medications on Admission, and Discharge Diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9550ea20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37344, 7)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings_to_search = ['Social History', 'Medications on Admission', 'Discharge Diagnosis']\n",
    "discharge_filter = discharge_summary[\"TEXT\"].apply(lambda text: \\\n",
    "                                                   all([pd.Series(text, dtype='str').str.contains(s, case=False).any() \\\n",
    "                                                        for s in strings_to_search]))\n",
    "final_discharge_summary = discharge_summary[discharge_filter]\n",
    "final_discharge_summary.reset_index(drop=True, inplace=True)\n",
    "final_discharge_summary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73b0802a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36948, 7)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_discharge_summary = final_discharge_summary.drop_duplicates(subset=\"HADM_ID\", keep = \"last\")\n",
    "final_discharge_summary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "903b8333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>177</td>\n",
       "      <td>13702</td>\n",
       "      <td>196489</td>\n",
       "      <td>2124-08-18</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2124-7-21**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>178</td>\n",
       "      <td>26880</td>\n",
       "      <td>135453</td>\n",
       "      <td>2162-03-25</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2162-3-3**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>179</td>\n",
       "      <td>53181</td>\n",
       "      <td>170490</td>\n",
       "      <td>2172-03-08</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2172-3-5**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180</td>\n",
       "      <td>20646</td>\n",
       "      <td>134727</td>\n",
       "      <td>2112-12-10</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2112-12-8**]              ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ROW_ID SUBJECT_ID HADM_ID   CHARTDATE           CATEGORY DESCRIPTION  \\\n",
       "0    176      13702  167118  2119-05-25  Discharge summary      Report   \n",
       "1    177      13702  196489  2124-08-18  Discharge summary      Report   \n",
       "2    178      26880  135453  2162-03-25  Discharge summary      Report   \n",
       "3    179      53181  170490  2172-03-08  Discharge summary      Report   \n",
       "4    180      20646  134727  2112-12-10  Discharge summary      Report   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Admission Date:  [**2119-5-4**]              D...  \n",
       "1  Admission Date:  [**2124-7-21**]              ...  \n",
       "2  Admission Date:  [**2162-3-3**]              D...  \n",
       "3  Admission Date:  [**2172-3-5**]              D...  \n",
       "4  Admission Date:  [**2112-12-8**]              ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_discharge_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2137fe17",
   "metadata": {},
   "source": [
    "#### Preporcess TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "607a6d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadm_symptom_mapping_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6815b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_in_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    content = line.split(\"|\")\n",
    "                    hadm_id = filename.split(\".\")[0]\n",
    "                    if hadm_id == content[0].split(\".\")[0]:\n",
    "                        hadm_symptom_mapping_list.append((hadm_id, (content[3])))\n",
    "                            \n",
    "folder_path = 'discharge_summary_files/full_input'\n",
    "read_files_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a672069a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>UMLS_CONCEPT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134788</td>\n",
       "      <td>Dyspnea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134788</td>\n",
       "      <td>Pulmonary Emphysema</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134788</td>\n",
       "      <td>Chronic Kidney Diseases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>134788</td>\n",
       "      <td>Myalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134788</td>\n",
       "      <td>Hemoptysis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  HADM_ID             UMLS_CONCEPT\n",
       "0  134788                  Dyspnea\n",
       "1  134788      Pulmonary Emphysema\n",
       "2  134788  Chronic Kidney Diseases\n",
       "3  134788                  Myalgia\n",
       "4  134788               Hemoptysis"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_mmap = pd.DataFrame(hadm_symptom_mapping_list, columns=[\"HADM_ID\", \"UMLS_CONCEPT\"])\n",
    "patient_mmap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7a13f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1766446, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_mmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8e62f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_umls = pd.merge(final_discharge_summary, patient_mmap, on=\"HADM_ID\", how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2faf38b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>UMLS_CONCEPT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "      <td>Pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "      <td>Hiatal Hernia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "      <td>Wheezing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "      <td>Erythema</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "      <td>Tracheomalacia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ROW_ID SUBJECT_ID HADM_ID   CHARTDATE           CATEGORY DESCRIPTION  \\\n",
       "0    176      13702  167118  2119-05-25  Discharge summary      Report   \n",
       "1    176      13702  167118  2119-05-25  Discharge summary      Report   \n",
       "2    176      13702  167118  2119-05-25  Discharge summary      Report   \n",
       "3    176      13702  167118  2119-05-25  Discharge summary      Report   \n",
       "4    176      13702  167118  2119-05-25  Discharge summary      Report   \n",
       "\n",
       "                                                TEXT    UMLS_CONCEPT  \n",
       "0  Admission Date:  [**2119-5-4**]              D...            Pain  \n",
       "1  Admission Date:  [**2119-5-4**]              D...   Hiatal Hernia  \n",
       "2  Admission Date:  [**2119-5-4**]              D...        Wheezing  \n",
       "3  Admission Date:  [**2119-5-4**]              D...        Erythema  \n",
       "4  Admission Date:  [**2119-5-4**]              D...  Tracheomalacia  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_umls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266522b",
   "metadata": {},
   "source": [
    "##### Further preprocessing\n",
    "Drop CATEGORY, DESCRIPTION column as they have only one unique value\n",
    "Category -> Discharge summary and Description -> Report\n",
    "Drop text column as we no longer need it we have extracted the UMLS concepts from MMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "197a01db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>UMLS_CONCEPT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>Pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>Hiatal Hernia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>Wheezing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>Erythema</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176</td>\n",
       "      <td>13702</td>\n",
       "      <td>167118</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>Tracheomalacia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ROW_ID SUBJECT_ID HADM_ID   CHARTDATE    UMLS_CONCEPT\n",
       "0    176      13702  167118  2119-05-25            Pain\n",
       "1    176      13702  167118  2119-05-25   Hiatal Hernia\n",
       "2    176      13702  167118  2119-05-25        Wheezing\n",
       "3    176      13702  167118  2119-05-25        Erythema\n",
       "4    176      13702  167118  2119-05-25  Tracheomalacia"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_umls = patient_umls.drop(columns = [\"CATEGORY\", \"DESCRIPTION\", \"TEXT\"])\n",
    "patient_umls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e06b7baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>UMLS_CONCEPT</th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>SYMPTOM_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>[Chronic Kidney Diseases, Diabetic Ketoacidosi...</td>\n",
       "      <td>42102</td>\n",
       "      <td>58526</td>\n",
       "      <td>2117-09-17</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>[Hepatitis C, Pain, Liver Cirrhosis, Esophagea...</td>\n",
       "      <td>19215</td>\n",
       "      <td>54610</td>\n",
       "      <td>2150-04-21</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100007</td>\n",
       "      <td>[Pain, Pneumonia, Hematuria, Abdominal Pain, D...</td>\n",
       "      <td>50238</td>\n",
       "      <td>23018</td>\n",
       "      <td>2145-04-07</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100009</td>\n",
       "      <td>[Pain, Coronary Artery Disease, Tachycardia, V...</td>\n",
       "      <td>21119</td>\n",
       "      <td>533</td>\n",
       "      <td>2162-05-21</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100010</td>\n",
       "      <td>[Pain, Hematuria, Renal Cell Carcinoma, Urinar...</td>\n",
       "      <td>40054</td>\n",
       "      <td>55853</td>\n",
       "      <td>2109-12-14</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  HADM_ID                                       UMLS_CONCEPT ROW_ID  \\\n",
       "0  100001  [Chronic Kidney Diseases, Diabetic Ketoacidosi...  42102   \n",
       "1  100003  [Hepatitis C, Pain, Liver Cirrhosis, Esophagea...  19215   \n",
       "2  100007  [Pain, Pneumonia, Hematuria, Abdominal Pain, D...  50238   \n",
       "3  100009  [Pain, Coronary Artery Disease, Tachycardia, V...  21119   \n",
       "4  100010  [Pain, Hematuria, Renal Cell Carcinoma, Urinar...  40054   \n",
       "\n",
       "  SUBJECT_ID   CHARTDATE  SYMPTOM_COUNT  \n",
       "0      58526  2117-09-17             44  \n",
       "1      54610  2150-04-21             51  \n",
       "2      23018  2145-04-07             46  \n",
       "3        533  2162-05-21             37  \n",
       "4      55853  2109-12-14             32  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symptom_counts = patient_umls['UMLS_CONCEPT'].value_counts()\n",
    "filtered_symptoms = symptom_counts[symptom_counts >= 10].index\n",
    "filtered_symptoms = set(filtered_symptoms) - {'symptom', 'illness', 'Communicable Diseases'}\n",
    "\n",
    "filtered_patient_umls = patient_umls[patient_umls['UMLS_CONCEPT'].isin(filtered_symptoms)]\n",
    "\n",
    "# Group by HADM_ID and aggregate UMLS_CONCEPTs as lists\n",
    "grouped_patient_umls = filtered_patient_umls.groupby('HADM_ID')\\\n",
    "                                            .agg({'UMLS_CONCEPT': list, \\\n",
    "                                                  'ROW_ID': 'first', \\\n",
    "                                                  'SUBJECT_ID': 'first', \\\n",
    "                                                  'CHARTDATE': 'first'})\n",
    "\n",
    "# Filter discharge summaries with symptoms within the range of 2 to 50\n",
    "grouped_patient_umls['SYMPTOM_COUNT'] = grouped_patient_umls['UMLS_CONCEPT'].apply(len)\n",
    "filtered_grouped_patient_umls = grouped_patient_umls[grouped_patient_umls['SYMPTOM_COUNT'] >= 2]\n",
    "\n",
    "# Retain the first 50 symptoms for summaries with more than 50 symptoms\n",
    "def retain_first_50(symptoms):\n",
    "    return symptoms[:50]\n",
    "\n",
    "filtered_grouped_patient_umls['UMLS_CONCEPT'] = filtered_grouped_patient_umls['UMLS_CONCEPT'].apply(retain_first_50)\n",
    "\n",
    "# Reset the index after filtering\n",
    "filtered_grouped_patient_umls.reset_index(inplace=True)\n",
    "filtered_grouped_patient_umls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f87a1cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36948, 6)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_grouped_patient_umls.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16364e63",
   "metadata": {},
   "source": [
    "#### Creating symptom vector representations:\n",
    "* Represent the extracted symptoms using the TF-IDF model and Word2Vec. You may use libraries like Scikit-learn for TF-IDF and Gensim for Word2Vec.\n",
    "* Train the Word2Vec skip-gram model on the raw discharge summaries with a window size of 5 and a vector dimension of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe556a87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 31815530)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = filtered_grouped_patient_umls['UMLS_CONCEPT'].apply(lambda x: ' '.join(x)).tolist()\n",
    "corpus_raw = ' '.join(corpus)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "idf = tfidf_vectorizer.idf_\n",
    "tfidf_weights = dict(zip(tfidf_vectorizer.get_feature_names_out(), idf))\n",
    "\n",
    "# b. Train the Word2Vec\n",
    "raw_sentences = simple_preprocess(corpus_raw)\n",
    "raw_w2v_model = Word2Vec(window=5, vector_size=100, min_count=3)\n",
    "raw_w2v_model.build_vocab([raw_sentences], progress_per=10000)\n",
    "raw_w2v_model.train([raw_sentences], total_examples=raw_w2v_model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b8c1f1",
   "metadata": {},
   "source": [
    "We will now define a function that uses the word2vec model and the TFIDF model to obtain our vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3dbbf8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/300185627.py:23: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_2a19nf9hj1/croot/pytorch_1675190251927/work/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  return torch.mean(torch.tensor(vectors),axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.1625,  0.6104, -0.0506,  0.2440,  0.2059, -0.7830,  0.3284,  0.9158,\n",
       "        -0.4759, -0.2274, -0.1188, -0.8359, -0.0753,  0.3254, -0.0245, -0.4491,\n",
       "         0.2411, -0.5197, -0.1897, -0.9636,  0.2608,  0.2770,  0.2923, -0.1891,\n",
       "         0.0220,  0.0795, -0.2599, -0.2148, -0.2935,  0.0196,  0.2488,  0.0355,\n",
       "         0.1956, -0.4394, -0.3878,  0.4624,  0.0748, -0.3770, -0.2339, -0.7320,\n",
       "         0.0509, -0.4920, -0.2550,  0.0464,  0.4831, -0.1799, -0.3402, -0.3869,\n",
       "         0.3908,  0.1591,  0.1792, -0.4478,  0.1075,  0.0081, -0.1708,  0.1481,\n",
       "         0.1791,  0.0978, -0.4154,  0.3576,  0.1425, -0.1824,  0.1595, -0.1279,\n",
       "        -0.4715,  0.3380,  0.2976,  0.5599, -0.7242,  0.6797, -0.2643,  0.3359,\n",
       "         0.7137, -0.1571,  0.5418,  0.1156,  0.0456, -0.1833, -0.4738,  0.0966,\n",
       "        -0.3462,  0.1245, -0.6028,  0.5132, -0.1917, -0.0763,  0.3530,  0.5156,\n",
       "         0.4589, -0.0204,  0.6099,  0.3594, -0.0545,  0.1349,  0.7873,  0.4803,\n",
       "         0.3620, -0.2243,  0.1528, -0.0153])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_output_vector(phrase):\n",
    "    words = nltk.word_tokenize(phrase)\n",
    "    #word2vec_vectors = []\n",
    "    #tfidf_vectors = []\n",
    "    vectors = []\n",
    "    has_value=False\n",
    "    for word in words:\n",
    "        if any(w.isalpha() for w in word):\n",
    "            word = word.lower()\n",
    "            try:\n",
    "                word2vec_vector = raw_w2v_model.wv.get_vector(word)\n",
    "                tfidf_vector = tfidf_weights[word]\n",
    "                op_vector = word2vec_vector * tfidf_vector\n",
    "                #word2vec_vectors.append(word2vec_vector)\n",
    "                #tfidf_vectors.append(tfidf_vector)\n",
    "                vectors.append(op_vector)\n",
    "            except KeyError:\n",
    "                continue\n",
    "            else:\n",
    "                has_value=True\n",
    "    # return np.mean(np.array(word2vec_vectors),axis=0),np.mean(np.array(tfidf_vectors),axis=0)\n",
    "    if has_value:\n",
    "        return torch.mean(torch.tensor(vectors),axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "get_output_vector('Acute appendicitis with perforation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f392ef",
   "metadata": {},
   "source": [
    "#### Preparing the dataset for multi-label classification:\n",
    "* Perform disease inference tasks for the 50 most common diseases and 100 most common diseases separately. Extract the respective discharge summaries for both tasks.\n",
    "* Divide the datasets into training (80%), validation (10%), and test (10%) sets randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9082b514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>UMLS_CONCEPT</th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>SYMPTOM_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>[Chronic Kidney Diseases, Diabetic Ketoacidosi...</td>\n",
       "      <td>42102</td>\n",
       "      <td>58526</td>\n",
       "      <td>2117-09-17</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>[Hepatitis C, Pain, Liver Cirrhosis, Esophagea...</td>\n",
       "      <td>19215</td>\n",
       "      <td>54610</td>\n",
       "      <td>2150-04-21</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100007</td>\n",
       "      <td>[Pain, Pneumonia, Hematuria, Abdominal Pain, D...</td>\n",
       "      <td>50238</td>\n",
       "      <td>23018</td>\n",
       "      <td>2145-04-07</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100009</td>\n",
       "      <td>[Pain, Coronary Artery Disease, Tachycardia, V...</td>\n",
       "      <td>21119</td>\n",
       "      <td>533</td>\n",
       "      <td>2162-05-21</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100011</td>\n",
       "      <td>[Mucolipidosis Type IV, Toxic Epidermal Necrol...</td>\n",
       "      <td>39781</td>\n",
       "      <td>87977</td>\n",
       "      <td>2177-09-12</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                       UMLS_CONCEPT ROW_ID  \\\n",
       "0   100001  [Chronic Kidney Diseases, Diabetic Ketoacidosi...  42102   \n",
       "1   100003  [Hepatitis C, Pain, Liver Cirrhosis, Esophagea...  19215   \n",
       "2   100007  [Pain, Pneumonia, Hematuria, Abdominal Pain, D...  50238   \n",
       "3   100009  [Pain, Coronary Artery Disease, Tachycardia, V...  21119   \n",
       "5   100011  [Mucolipidosis Type IV, Toxic Epidermal Necrol...  39781   \n",
       "\n",
       "  SUBJECT_ID   CHARTDATE  SYMPTOM_COUNT  \n",
       "0      58526  2117-09-17             44  \n",
       "1      54610  2150-04-21             51  \n",
       "2      23018  2145-04-07             46  \n",
       "3        533  2162-05-21             37  \n",
       "5      87977  2177-09-12             23  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load diagnoses file\n",
    "diagnoses_icd = pd.read_csv('DIAGNOSES_ICD.csv.gz')\n",
    "\n",
    "# Get top 50 ICD9 codes\n",
    "top50_icd9 = diagnoses_icd.groupby([\"ICD9_CODE\"]).count().sort_values(\"ROW_ID\", ascending = False)[: 50].index\n",
    "top100_icd9 = diagnoses_icd.groupby([\"ICD9_CODE\"]).count().sort_values(\"ROW_ID\", ascending = False)[: 100].index\n",
    "all_icd=diagnoses_icd.groupby([\"ICD9_CODE\"]).count().sort_values(\"ROW_ID\", ascending = False).index\n",
    "\n",
    "# Get HADM_ID which has top 50 ICD9 codes\n",
    "top50_hadmid = diagnoses_icd[diagnoses_icd[\"ICD9_CODE\"].isin(top50_icd9)][\"HADM_ID\"].unique()\n",
    "top100_hadmid = diagnoses_icd[diagnoses_icd[\"ICD9_CODE\"].isin(top100_icd9)][\"HADM_ID\"].unique()\n",
    "\n",
    "# Filter data from filtered_grouped_patient_umls with only those HADM_ID\n",
    "filtered_grouped_patient_umls[\"HADM_ID\"] = filtered_grouped_patient_umls[\"HADM_ID\"].astype('int64')\n",
    "top_50_patient_umls = filtered_grouped_patient_umls[filtered_grouped_patient_umls[\"HADM_ID\"].isin(top50_hadmid)]\n",
    "top_100_patient_umls = filtered_grouped_patient_umls[filtered_grouped_patient_umls[\"HADM_ID\"].isin(top100_hadmid)]\n",
    "\n",
    "top_50_patient_umls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9272ddac",
   "metadata": {},
   "source": [
    "We will now merge the patient_umls containing the top 50/top 100 diseases dataframe with the diagnoses dataframe, which contains the corresponding ICD9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78e36582",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses_icd_grouped = diagnoses_icd.groupby(\"HADM_ID\")[\"ICD9_CODE\"].agg(list).reset_index()\n",
    "top_50_patient_umls = pd.merge(top_50_patient_umls, diagnoses_icd_grouped, on = [\"HADM_ID\"], how = \"inner\")\n",
    "top_100_patient_umls = pd.merge(top_100_patient_umls, diagnoses_icd_grouped, on = [\"HADM_ID\"], how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca2a498a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35003, 7)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_50_patient_umls.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f474350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35744, 7)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100_patient_umls.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ac8d5",
   "metadata": {},
   "source": [
    "Now that we have the top 50 and top 100 data, let us split the dataset into training, validation and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94473c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "    \n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    val_data.reset_index(drop=True, inplace=True)\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # GEt only the required columns\n",
    "    required_columns = ['HADM_ID', 'UMLS_CONCEPT', 'ICD9_CODE']\n",
    "    \n",
    "    return train_data[required_columns], val_data[required_columns], test_data[required_columns]\n",
    "\n",
    "train_50, val_50, test_50 = split_data(top_50_patient_umls)\n",
    "train_100, val_100, test_100 = split_data(top_100_patient_umls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "65394633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>UMLS_CONCEPT</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>168925</td>\n",
       "      <td>[Renal Cell Carcinoma, Renal carcinoma, Pneumo...</td>\n",
       "      <td>[9951, 486, 1970, 1890, 19889, 28860, 40390, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174693</td>\n",
       "      <td>[Toxic Epidermal Necrolysis, Abdominal Pain, E...</td>\n",
       "      <td>[00845, 4321, 4019, 2720, 2899, 78079]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>192718</td>\n",
       "      <td>[Right bundle branch block, Chronic Kidney Dis...</td>\n",
       "      <td>[4271, 42823, 5849, 2762, 2851, 25000, 4280, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102253</td>\n",
       "      <td>[Diabetic Ketoacidosis, Fibromyalgia, Diabetes...</td>\n",
       "      <td>[99659, 25013, V5867, E8781, 78659, 79431, 729...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101363</td>\n",
       "      <td>[Pain, Hematuria, Dyspnea, Back Pain, Ileus, R...</td>\n",
       "      <td>[4417, 41511, 5849, 5601, 2851, 4019]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                       UMLS_CONCEPT  \\\n",
       "0   168925  [Renal Cell Carcinoma, Renal carcinoma, Pneumo...   \n",
       "1   174693  [Toxic Epidermal Necrolysis, Abdominal Pain, E...   \n",
       "2   192718  [Right bundle branch block, Chronic Kidney Dis...   \n",
       "3   102253  [Diabetic Ketoacidosis, Fibromyalgia, Diabetes...   \n",
       "4   101363  [Pain, Hematuria, Dyspnea, Back Pain, Ileus, R...   \n",
       "\n",
       "                                           ICD9_CODE  \n",
       "0  [9951, 486, 1970, 1890, 19889, 28860, 40390, 5...  \n",
       "1             [00845, 4321, 4019, 2720, 2899, 78079]  \n",
       "2  [4271, 42823, 5849, 2762, 2851, 25000, 4280, 4...  \n",
       "3  [99659, 25013, V5867, E8781, 78659, 79431, 729...  \n",
       "4              [4417, 41511, 5849, 5601, 2851, 4019]  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_50.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bcfe72d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28002, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e04c0466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>UMLS_CONCEPT</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>168925</td>\n",
       "      <td>[Renal Cell Carcinoma, Renal carcinoma, Pneumo...</td>\n",
       "      <td>[9951, 486, 1970, 1890, 19889, 28860, 40390, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174693</td>\n",
       "      <td>[Toxic Epidermal Necrolysis, Abdominal Pain, E...</td>\n",
       "      <td>[00845, 4321, 4019, 2720, 2899, 78079]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>192718</td>\n",
       "      <td>[Right bundle branch block, Chronic Kidney Dis...</td>\n",
       "      <td>[4271, 42823, 5849, 2762, 2851, 25000, 4280, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102253</td>\n",
       "      <td>[Diabetic Ketoacidosis, Fibromyalgia, Diabetes...</td>\n",
       "      <td>[99659, 25013, V5867, E8781, 78659, 79431, 729...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101363</td>\n",
       "      <td>[Pain, Hematuria, Dyspnea, Back Pain, Ileus, R...</td>\n",
       "      <td>[4417, 41511, 5849, 5601, 2851, 4019]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                       UMLS_CONCEPT  \\\n",
       "0   168925  [Renal Cell Carcinoma, Renal carcinoma, Pneumo...   \n",
       "1   174693  [Toxic Epidermal Necrolysis, Abdominal Pain, E...   \n",
       "2   192718  [Right bundle branch block, Chronic Kidney Dis...   \n",
       "3   102253  [Diabetic Ketoacidosis, Fibromyalgia, Diabetes...   \n",
       "4   101363  [Pain, Hematuria, Dyspnea, Back Pain, Ileus, R...   \n",
       "\n",
       "                                           ICD9_CODE  \n",
       "0  [9951, 486, 1970, 1890, 19889, 28860, 40390, 5...  \n",
       "1             [00845, 4321, 4019, 2720, 2899, 78079]  \n",
       "2  [4271, 42823, 5849, 2762, 2851, 25000, 4280, 4...  \n",
       "3  [99659, 25013, V5867, E8781, 78659, 79431, 729...  \n",
       "4              [4417, 41511, 5849, 5601, 2851, 4019]  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_50.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069cfe6e",
   "metadata": {},
   "source": [
    "## Training and evaluating the model:\n",
    "* Train the BiLSTM model using the binary cross-entropy loss function and the Adam optimizer with a learning rate of 0.001 and a batch size of 400. Set the hidden node size to 100 and use a dynamical mechanism with 50 epochs and a dropout rate of 0.8.\n",
    "* Train and evaluate the model using TF-IDF+Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b879c95",
   "metadata": {},
   "source": [
    "Use the created train, validation, and test sets that have been created above namely `train_50`, `train_100`, `val_50`, `val_100`, `test_50`, `test_100` and run the desired model over them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea34c7be",
   "metadata": {},
   "source": [
    "### Running BiLSTM on top 50 ICD codes\n",
    "Lets start by preparing the dataset that contains diseases filtered from top 50 ICD9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e147379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = []\n",
    "        for idx in range(len(self.X)):\n",
    "            row_x = self.X.iloc[idx]\n",
    "            row_y = y[y['HADM_ID']==row_x['HADM_ID']]\n",
    "            self.y.append(row_y)\n",
    "        self.y = np.array(self.y)\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row_x = self.X.iloc[idx]\n",
    "        row_y = self.y[idx]\n",
    "        return row_x['UMLS_CONCEPT'], row_y['ICD9_CODE'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9939eff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/2944304301.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.y = np.array(self.y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28002, 3)\n",
      "(3500, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Coronary Arteriosclerosis',\n",
       "  'Coronary Artery Disease',\n",
       "  'Acute Kidney Insufficiency',\n",
       "  'Kidney Failure, Acute',\n",
       "  'Atrial Fibrillation',\n",
       "  'Leukocytosis',\n",
       "  'Diabetes Mellitus',\n",
       "  'Hypercholesterolemia',\n",
       "  'Pain',\n",
       "  'Disease',\n",
       "  'Diabetes Mellitus, Non-Insulin-Dependent',\n",
       "  'Discharge, body substance',\n",
       "  'Erythema',\n",
       "  'Fever',\n",
       "  'Acute GVH disease',\n",
       "  'Dyspnea on exertion',\n",
       "  'Hypertensive disease',\n",
       "  'chronic back pain',\n",
       "  'Acute respiratory failure',\n",
       "  'Complaint (finding)',\n",
       "  'Illness (finding)',\n",
       "  'Primary Neoplasm',\n",
       "  'SHORT STATURE, ONYCHODYSPLASIA, FACIAL DYSMORPHISM, AND HYPOTRICHOSIS SYNDROME',\n",
       "  'Three Vessel Coronary Disease',\n",
       "  'chest pain radiating to neck'],\n",
       " array(['41401', '5849', '4240', '4019', '42731', '25000', '2720'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = MultiLabelDataset(train_50,diagnoses_icd[diagnoses_icd[\"ICD9_CODE\"].isin(top50_icd9.values)])\n",
    "val_dataset = MultiLabelDataset(val_50,diagnoses_icd[diagnoses_icd[\"ICD9_CODE\"].isin(top50_icd9.values)])\n",
    "print(train_50.shape)\n",
    "print(val_50.shape)\n",
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f1eb86",
   "metadata": {},
   "source": [
    "As we can see, our dataset contains a the string of words as the predictor and the numpy array as the label. Lets now create a collate function that will load a batch of data, perform word2vec embedding on them and return a padded standard numpy array as the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a52099d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 50, 100])\n",
      "torch.Size([400, 50, 100])\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    \n",
    "    keywords_batch, labels = zip(*data)\n",
    "    sequences = []\n",
    "    for keywords in keywords_batch:\n",
    "        vectors = []\n",
    "        for phrase in keywords:\n",
    "            vector = get_output_vector(phrase)\n",
    "            if vector is None:\n",
    "                continue\n",
    "            vectors.append(vector)\n",
    "        sequences.append(np.array(vectors))\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.float32)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.float32)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            \"\"\"\n",
    "            TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "            \"\"\"\n",
    "            cur_patient_len = len(patient)\n",
    "            x[i_patient][j_visit] = visit\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=400, collate_fn=collate_fn,shuffle=True)\n",
    "loader_iter = iter(train_loader)\n",
    "print(next(loader_iter)[0].shape)\n",
    "val_loader = DataLoader(val_dataset, batch_size=400, collate_fn=collate_fn,shuffle=True)\n",
    "loader_iter = iter(val_loader)\n",
    "print(next(loader_iter)[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e5d126",
   "metadata": {},
   "source": [
    "### Creating the neural Network\n",
    "Great! Our dataset can now be loaded in batches. Let us now make use of it in our neural network. the network is going to be a BiLSTM netork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "60918b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        self.softmax = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = self.dropout(lstm_out[:, -1, :])\n",
    "        output = self.fc(lstm_out)\n",
    "        return self.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "061c95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100 # Embedding size of word2vec\n",
    "output_shape = 50 # Number of diseases\n",
    "\n",
    "model = BiLSTM(hidden_size, hidden_size, output_shape)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f352b6",
   "metadata": {},
   "source": [
    "#### Train the neural network\n",
    "Our setup is now ready. We can now see the neural network in action! Lets start with extracting the set of ICD9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "96d21fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "icd9_codes = top50_icd9.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe81a2",
   "metadata": {},
   "source": [
    "In each epoch, we will -\n",
    "- Train the neural network on the shuffled dataset\n",
    "- Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9fd6e374",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Dataset Processed: 200, Training Loss for the batch: 17.037996\n",
      "Epoch: 1 \t Training Loss (of the entire dataset): 0.085207\t Time to train: 253.76377701759338\n",
      "Epoch: 1, Validation Loss: 0.0815\n",
      "Epoch: 2, Dataset Processed: 400, Training Loss for the batch: 10.035663\n",
      "Epoch: 2 \t Training Loss (of the entire dataset): 0.084196\t Time to train: 246.2989320755005\n",
      "Epoch: 2, Validation Loss: 0.0813\n",
      "Epoch: 3, Dataset Processed: 600, Training Loss for the batch: 3.192108\n",
      "Epoch: 3, Dataset Processed: 800, Training Loss for the batch: 16.751814\n",
      "Epoch: 3 \t Training Loss (of the entire dataset): 0.083814\t Time to train: 248.0542929172516\n",
      "Epoch: 3, Validation Loss: 0.0817\n",
      "Epoch: 4, Dataset Processed: 1000, Training Loss for the batch: 13.121998\n",
      "Epoch: 4 \t Training Loss (of the entire dataset): 0.083399\t Time to train: 256.4335460662842\n",
      "Epoch: 4, Validation Loss: 0.0799\n",
      "Epoch: 5, Dataset Processed: 1200, Training Loss for the batch: 6.338805\n",
      "Epoch: 5, Dataset Processed: 1400, Training Loss for the batch: 16.570197\n",
      "Epoch: 5 \t Training Loss (of the entire dataset): 0.083034\t Time to train: 265.0789806842804\n",
      "Epoch: 5, Validation Loss: 0.0794\n",
      "Epoch: 6, Dataset Processed: 1600, Training Loss for the batch: 16.131550\n",
      "Epoch: 6 \t Training Loss (of the entire dataset): 0.082823\t Time to train: 250.9350860118866\n",
      "Epoch: 6, Validation Loss: 0.0793\n",
      "Epoch: 7, Dataset Processed: 1800, Training Loss for the batch: 9.420301\n",
      "Epoch: 7 \t Training Loss (of the entire dataset): 0.082493\t Time to train: 245.575865983963\n",
      "Epoch: 7, Validation Loss: 0.0795\n",
      "Epoch: 8, Dataset Processed: 2000, Training Loss for the batch: 2.746901\n",
      "Epoch: 8, Dataset Processed: 2200, Training Loss for the batch: 16.431910\n",
      "Epoch: 8 \t Training Loss (of the entire dataset): 0.082186\t Time to train: 246.64731192588806\n",
      "Epoch: 8, Validation Loss: 0.0791\n",
      "Epoch: 9, Dataset Processed: 2400, Training Loss for the batch: 12.509479\n",
      "Epoch: 9 \t Training Loss (of the entire dataset): 0.082130\t Time to train: 251.0609369277954\n",
      "Epoch: 9, Validation Loss: 0.0787\n",
      "Epoch: 10, Dataset Processed: 2600, Training Loss for the batch: 5.812555\n",
      "Epoch: 10, Dataset Processed: 2800, Training Loss for the batch: 16.363694\n",
      "Epoch: 10 \t Training Loss (of the entire dataset): 0.081839\t Time to train: 257.37538409233093\n",
      "Epoch: 10, Validation Loss: 0.0783\n",
      "Epoch: 11, Dataset Processed: 3000, Training Loss for the batch: 15.499341\n",
      "Epoch: 11 \t Training Loss (of the entire dataset): 0.081584\t Time to train: 248.41862297058105\n",
      "Epoch: 11, Validation Loss: 0.0784\n",
      "Epoch: 12, Dataset Processed: 3200, Training Loss for the batch: 8.910519\n",
      "Epoch: 12 \t Training Loss (of the entire dataset): 0.081563\t Time to train: 245.5705738067627\n",
      "Epoch: 12, Validation Loss: 0.0776\n",
      "Epoch: 13, Dataset Processed: 3400, Training Loss for the batch: 2.282767\n",
      "Epoch: 13, Dataset Processed: 3600, Training Loss for the batch: 16.266389\n",
      "Epoch: 13 \t Training Loss (of the entire dataset): 0.081183\t Time to train: 246.35519003868103\n",
      "Epoch: 13, Validation Loss: 0.0785\n",
      "Epoch: 14, Dataset Processed: 3800, Training Loss for the batch: 11.946129\n",
      "Epoch: 14 \t Training Loss (of the entire dataset): 0.081041\t Time to train: 250.9112479686737\n",
      "Epoch: 14, Validation Loss: 0.0776\n",
      "Epoch: 15, Dataset Processed: 4000, Training Loss for the batch: 5.324930\n",
      "Epoch: 15, Dataset Processed: 4200, Training Loss for the batch: 16.121717\n",
      "Epoch: 15 \t Training Loss (of the entire dataset): 0.080605\t Time to train: 245.00755977630615\n",
      "Epoch: 15, Validation Loss: 0.0782\n",
      "Epoch: 16, Dataset Processed: 4400, Training Loss for the batch: 14.895955\n",
      "Epoch: 16 \t Training Loss (of the entire dataset): 0.080448\t Time to train: 244.52511405944824\n",
      "Epoch: 16, Validation Loss: 0.0772\n",
      "Epoch: 17, Dataset Processed: 4600, Training Loss for the batch: 8.358183\n",
      "Epoch: 17 \t Training Loss (of the entire dataset): 0.080191\t Time to train: 248.08755016326904\n",
      "Epoch: 17, Validation Loss: 0.0767\n",
      "Epoch: 18, Dataset Processed: 4800, Training Loss for the batch: 1.832647\n",
      "Epoch: 18, Dataset Processed: 5000, Training Loss for the batch: 16.001284\n",
      "Epoch: 18 \t Training Loss (of the entire dataset): 0.080045\t Time to train: 249.01916313171387\n",
      "Epoch: 18, Validation Loss: 0.0798\n",
      "Epoch: 19, Dataset Processed: 5200, Training Loss for the batch: 11.452805\n",
      "Epoch: 19 \t Training Loss (of the entire dataset): 0.080284\t Time to train: 250.16337203979492\n",
      "Epoch: 19, Validation Loss: 0.0764\n",
      "Epoch: 20, Dataset Processed: 5400, Training Loss for the batch: 4.863357\n",
      "Epoch: 20, Dataset Processed: 5600, Training Loss for the batch: 15.989281\n",
      "Epoch: 20 \t Training Loss (of the entire dataset): 0.079876\t Time to train: 245.1073338985443\n",
      "Epoch: 20, Validation Loss: 0.0765\n",
      "Epoch: 21, Dataset Processed: 5800, Training Loss for the batch: 14.358348\n",
      "Epoch: 21 \t Training Loss (of the entire dataset): 0.079687\t Time to train: 246.64045119285583\n",
      "Epoch: 21, Validation Loss: 0.0764\n",
      "Epoch: 22, Dataset Processed: 6000, Training Loss for the batch: 7.916355\n",
      "Epoch: 22 \t Training Loss (of the entire dataset): 0.079634\t Time to train: 245.796147108078\n",
      "Epoch: 22, Validation Loss: 0.0769\n",
      "Epoch: 23, Dataset Processed: 6200, Training Loss for the batch: 1.445468\n",
      "Epoch: 23, Dataset Processed: 6400, Training Loss for the batch: 15.907965\n",
      "Epoch: 23 \t Training Loss (of the entire dataset): 0.079481\t Time to train: 257.81738209724426\n",
      "Epoch: 23, Validation Loss: 0.0765\n",
      "Epoch: 24, Dataset Processed: 6600, Training Loss for the batch: 10.919346\n",
      "Epoch: 24 \t Training Loss (of the entire dataset): 0.079555\t Time to train: 250.38722324371338\n",
      "Epoch: 24, Validation Loss: 0.0772\n",
      "Epoch: 25, Dataset Processed: 6800, Training Loss for the batch: 4.450808\n",
      "Epoch: 25, Dataset Processed: 7000, Training Loss for the batch: 15.811931\n",
      "Epoch: 25 \t Training Loss (of the entire dataset): 0.079170\t Time to train: 245.32820415496826\n",
      "Epoch: 25, Validation Loss: 0.0757\n",
      "Epoch: 26, Dataset Processed: 7200, Training Loss for the batch: 13.869277\n",
      "Epoch: 26 \t Training Loss (of the entire dataset): 0.079250\t Time to train: 245.12822771072388\n",
      "Epoch: 26, Validation Loss: 0.0837\n",
      "Epoch: 27, Dataset Processed: 7400, Training Loss for the batch: 7.862285\n",
      "Epoch: 27 \t Training Loss (of the entire dataset): 0.081419\t Time to train: 250.86124324798584\n",
      "Epoch: 27, Validation Loss: 0.0772\n",
      "Epoch: 28, Dataset Processed: 7600, Training Loss for the batch: 1.042596\n",
      "Epoch: 28, Dataset Processed: 7800, Training Loss for the batch: 15.995256\n",
      "Epoch: 28 \t Training Loss (of the entire dataset): 0.079764\t Time to train: 246.24697279930115\n",
      "Epoch: 28, Validation Loss: 0.0773\n",
      "Epoch: 29, Dataset Processed: 8000, Training Loss for the batch: 10.489636\n",
      "Epoch: 29 \t Training Loss (of the entire dataset): 0.079363\t Time to train: 244.95795392990112\n",
      "Epoch: 29, Validation Loss: 0.0763\n",
      "Epoch: 30, Dataset Processed: 8200, Training Loss for the batch: 4.058910\n",
      "Epoch: 30, Dataset Processed: 8400, Training Loss for the batch: 15.842188\n",
      "Epoch: 30 \t Training Loss (of the entire dataset): 0.079407\t Time to train: 249.75026392936707\n",
      "Epoch: 30, Validation Loss: 0.0762\n",
      "Epoch: 31, Dataset Processed: 8600, Training Loss for the batch: 13.438541\n",
      "Epoch: 31 \t Training Loss (of the entire dataset): 0.078870\t Time to train: 251.472243309021\n",
      "Epoch: 31, Validation Loss: 0.0758\n",
      "Epoch: 32, Dataset Processed: 8800, Training Loss for the batch: 6.997161\n",
      "Epoch: 32 \t Training Loss (of the entire dataset): 0.078785\t Time to train: 251.29036116600037\n",
      "Epoch: 32, Validation Loss: 0.0753\n",
      "Epoch: 33, Dataset Processed: 9000, Training Loss for the batch: 0.635147\n",
      "Epoch: 33, Dataset Processed: 9200, Training Loss for the batch: 15.684709\n",
      "Epoch: 33 \t Training Loss (of the entire dataset): 0.078594\t Time to train: 245.81308603286743\n",
      "Epoch: 33, Validation Loss: 0.0755\n",
      "Epoch: 34, Dataset Processed: 9400, Training Loss for the batch: 9.992440\n",
      "Epoch: 34 \t Training Loss (of the entire dataset): 0.078390\t Time to train: 250.2995080947876\n",
      "Epoch: 34, Validation Loss: 0.0751\n",
      "Epoch: 35, Dataset Processed: 9600, Training Loss for the batch: 3.617122\n",
      "Epoch: 35, Dataset Processed: 9800, Training Loss for the batch: 15.700166\n",
      "Epoch: 35 \t Training Loss (of the entire dataset): 0.078383\t Time to train: 243.1490089893341\n",
      "Epoch: 35, Validation Loss: 0.0761\n",
      "Epoch: 36, Dataset Processed: 10000, Training Loss for the batch: 12.909503\n",
      "Epoch: 36 \t Training Loss (of the entire dataset): 0.078268\t Time to train: 245.70653915405273\n",
      "Epoch: 36, Validation Loss: 0.0766\n",
      "Epoch: 37, Dataset Processed: 10200, Training Loss for the batch: 6.615642\n",
      "Epoch: 37 \t Training Loss (of the entire dataset): 0.078234\t Time to train: 255.70450687408447\n",
      "Epoch: 37, Validation Loss: 0.0751\n",
      "Epoch: 38, Dataset Processed: 10400, Training Loss for the batch: 0.235210\n",
      "Epoch: 38, Dataset Processed: 10600, Training Loss for the batch: 15.574841\n",
      "Epoch: 38 \t Training Loss (of the entire dataset): 0.078123\t Time to train: 253.87093591690063\n",
      "Epoch: 38, Validation Loss: 0.0785\n",
      "Epoch: 39, Dataset Processed: 10800, Training Loss for the batch: 9.594346\n",
      "Epoch: 39 \t Training Loss (of the entire dataset): 0.078128\t Time to train: 214.05973196029663\n",
      "Epoch: 39, Validation Loss: 0.0763\n",
      "Epoch: 40, Dataset Processed: 11000, Training Loss for the batch: 3.263439\n",
      "Epoch: 40, Dataset Processed: 11200, Training Loss for the batch: 15.552161\n",
      "Epoch: 40 \t Training Loss (of the entire dataset): 0.078077\t Time to train: 217.83492612838745\n",
      "Epoch: 40, Validation Loss: 0.0746\n",
      "Epoch: 41, Dataset Processed: 11400, Training Loss for the batch: 12.424038\n",
      "Epoch: 41 \t Training Loss (of the entire dataset): 0.077836\t Time to train: 211.68599605560303\n",
      "Epoch: 41, Validation Loss: 0.0764\n",
      "Epoch: 42, Dataset Processed: 11600, Training Loss for the batch: 6.156338\n",
      "Epoch: 42, Dataset Processed: 11800, Training Loss for the batch: 15.538562\n",
      "Epoch: 42 \t Training Loss (of the entire dataset): 0.077858\t Time to train: 210.95394086837769\n",
      "Epoch: 42, Validation Loss: 0.0758\n",
      "Epoch: 43, Dataset Processed: 12000, Training Loss for the batch: 15.413284\n",
      "Epoch: 43 \t Training Loss (of the entire dataset): 0.077702\t Time to train: 212.43765783309937\n",
      "Epoch: 43, Validation Loss: 0.0748\n",
      "Epoch: 44, Dataset Processed: 12200, Training Loss for the batch: 9.086649\n",
      "Epoch: 44 \t Training Loss (of the entire dataset): 0.077419\t Time to train: 214.97547698020935\n",
      "Epoch: 44, Validation Loss: 0.0750\n",
      "Epoch: 45, Dataset Processed: 12400, Training Loss for the batch: 2.781238\n",
      "Epoch: 45, Dataset Processed: 12600, Training Loss for the batch: 15.534604\n",
      "Epoch: 45 \t Training Loss (of the entire dataset): 0.077542\t Time to train: 217.55113911628723\n",
      "Epoch: 45, Validation Loss: 0.0746\n",
      "Epoch: 46, Dataset Processed: 12800, Training Loss for the batch: 11.986418\n",
      "Epoch: 46 \t Training Loss (of the entire dataset): 0.077276\t Time to train: 214.91508412361145\n",
      "Epoch: 46, Validation Loss: 0.0745\n",
      "Epoch: 47, Dataset Processed: 13000, Training Loss for the batch: 5.714025\n",
      "Epoch: 47, Dataset Processed: 13200, Training Loss for the batch: 15.463053\n",
      "Epoch: 47 \t Training Loss (of the entire dataset): 0.077244\t Time to train: 214.957026720047\n",
      "Epoch: 47, Validation Loss: 0.0743\n",
      "Epoch: 48, Dataset Processed: 13400, Training Loss for the batch: 14.927771\n",
      "Epoch: 48 \t Training Loss (of the entire dataset): 0.077411\t Time to train: 216.43276000022888\n",
      "Epoch: 48, Validation Loss: 0.0746\n",
      "Epoch: 49, Dataset Processed: 13600, Training Loss for the batch: 8.628827\n",
      "Epoch: 49 \t Training Loss (of the entire dataset): 0.077142\t Time to train: 220.2492458820343\n",
      "Epoch: 49, Validation Loss: 0.0773\n",
      "Epoch: 50, Dataset Processed: 13800, Training Loss for the batch: 2.521832\n",
      "Epoch: 50, Dataset Processed: 14000, Training Loss for the batch: 15.462526\n",
      "Epoch: 50 \t Training Loss (of the entire dataset): 0.077373\t Time to train: 216.31211400032043\n",
      "Epoch: 50, Validation Loss: 0.0749\n",
      "Time to train the dataset: {} 13083.0730240345\n"
     ]
    }
   ],
   "source": [
    "def train_bilstm_model(train_dataloader,val_dataloader):\n",
    "    i=0\n",
    "    start_time = time.time()\n",
    "    for epoch in range(100):\n",
    "        batch_start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        batch_train_loss = 0\n",
    "        for batch_X, batch_y in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch_X)\n",
    "            probs = []\n",
    "            for y_val in batch_y:\n",
    "                array_res = torch.from_numpy(npi.indices(icd9_codes,np.array(y_val)))\n",
    "                row_probs = torch.sum(nn.functional.one_hot(array_res,num_classes=output_shape),axis=0)\n",
    "                probs.append(row_probs)\n",
    "            probs = torch.stack(probs)\n",
    "            loss = criterion(y_pred, probs.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            batch_train_loss += loss.item()\n",
    "            i+=1\n",
    "            if i%200==0:\n",
    "                print(\"Epoch: {}, Dataset Processed: {}, Training Loss for the batch: {:.6f}\".format(epoch+1,i, batch_train_loss))\n",
    "                batch_train_loss=0\n",
    "        batch_end_time = time.time()\n",
    "        train_loss = train_loss / len(train_dataloader)\n",
    "        print('Epoch: {} \\t Training Loss (of the entire dataset): {:.6f}\\t Time to train: {}'.format(epoch+1, train_loss,batch_end_time-batch_start_time))\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        y_true = []\n",
    "        y_preds = []\n",
    "        for batch_X, batch_y in val_dataloader:\n",
    "            y_pred = model(batch_X)\n",
    "            probs = []\n",
    "            for y_val in batch_y:\n",
    "                array_res = torch.from_numpy(npi.indices(icd9_codes,np.array(y_val)))\n",
    "                row_probs = torch.sum(nn.functional.one_hot(array_res,num_classes=output_shape),axis=0)\n",
    "                probs.append(row_probs)\n",
    "            probs = torch.stack(probs)\n",
    "            y_true.append(probs)\n",
    "            y_preds.append(torch.where(y_pred>0.2,1,0))\n",
    "            loss = criterion(y_pred, probs.float())\n",
    "            val_loss += loss.item()\n",
    "        print(\"Epoch: {}, Validation Loss: {:.4f}\".format(epoch+1, val_loss/len(val_dataloader)))\n",
    "        \n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"Time to train the dataset: {}\",end_time-start_time)\n",
    "    \n",
    "train_bilstm_model(train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d53d8",
   "metadata": {},
   "source": [
    "### Testing the neural network\n",
    "Our BiLSTM model with top 50 ICD9 codes has now been built. Let us try and test with the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59805ab0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/2944304301.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.y = np.array(self.y)\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array(labels)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 50, 100])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MultiLabelDataset(test_50,diagnoses_icd[diagnoses_icd[\"ICD9_CODE\"].isin(top50_icd9.values)])\n",
    "test_loader = DataLoader(test_dataset, batch_size=50, collate_fn=collate_fn,shuffle=False)\n",
    "loader_iter = iter(test_loader)\n",
    "next(loader_iter)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2e10adcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 batches\n",
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3589351336.py:15: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  y_pred = np.array(y_pred_list)\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3589351336.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_pred = np.array(y_pred_list)\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3589351336.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_test = np.array(y_test_list)\n"
     ]
    }
   ],
   "source": [
    "def get_test_preds(test_dataloader):\n",
    "    \n",
    "    y_pred_list = []\n",
    "    y_test_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for batch_X, batch_y in test_dataloader:\n",
    "            y_pred = model(batch_X)\n",
    "            y_pred_list.append(y_pred)\n",
    "            y_test_list.append(batch_y)\n",
    "            if i%10==0:\n",
    "                print(\"Processed\",i,\"batches\")\n",
    "            i += 1\n",
    "    y_pred = np.array(y_pred_list)\n",
    "    y_test = np.array(y_test_list)\n",
    "\n",
    "    return y_test, y_pred\n",
    "y_test,y_pred=get_test_preds(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af6c2a",
   "metadata": {},
   "source": [
    "Now that we have the y_pred and y_true, let us match the original format of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fef1abcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3501, 50)\n"
     ]
    }
   ],
   "source": [
    "y_pred_numpy = []\n",
    "for pred_batch in y_pred:\n",
    "    for pred in pred_batch:\n",
    "        y_pred_numpy.append(pred.detach().numpy())\n",
    "y_pred_numpy = np.array(y_pred_numpy)\n",
    "print(y_pred_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "52cf63da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3501, 50)\n"
     ]
    }
   ],
   "source": [
    "y_test_numpy = []\n",
    "for test_batch in y_test:\n",
    "    for test in test_batch:\n",
    "        op = torch.from_numpy(npi.indices(icd9_codes,np.array(test)))\n",
    "        probs = nn.functional.one_hot(op,num_classes=y_pred_numpy.shape[1])\n",
    "        y_test_numpy.append(probs.detach().numpy()[0])\n",
    "y_test_numpy = np.array(y_test_numpy)\n",
    "print(y_test_numpy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ffd065",
   "metadata": {},
   "source": [
    "#### Metric evaluation\n",
    "Lets use this to evaluate how BiLSTM performs on top 50 ICD9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8edfb2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiP: 0.4540, MiR: 0.5710, MiF1: 0.5058, Micro AUC: 0.8200\n",
      "MaP: 0.4130, MaR: 0.4220, MaF1: 0.4175, Macro AUC: 0.7520\n"
     ]
    }
   ],
   "source": [
    "def evaluate(y_test, y_pred):\n",
    "    y_test = np.array(y_test)\n",
    "    threshold = 0.2\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "    \n",
    "    # Evaluation Metrics\n",
    "    mip = precision_score(y_test, y_pred_binary, average='micro')\n",
    "    mir = recall_score(y_test, y_pred_binary, average='micro')\n",
    "    mif1 = f1_score(y_test, y_pred_binary, average='micro')\n",
    "    micro_auc = roc_auc_score(y_test, y_pred, average='micro')\n",
    "    \n",
    "    no_class_idx = np.argwhere(np.all(y_test[..., :] == 0, axis=0))\n",
    "    y_test_cleaned = np.delete(y_test, no_class_idx, axis=1)\n",
    "    y_pred_cleaned = np.delete(y_pred, no_class_idx, axis=1)\n",
    "    y_pred_binary_cleaned = np.delete(y_pred_binary, no_class_idx, axis=1)\n",
    "\n",
    "    print(\"MiP: {:.4f}, MiR: {:.4f}, MiF1: {:.4f}, Micro AUC: {:.4f}\".format(mip, mir, mif1,micro_auc))\n",
    "    print(\"MaP: {:.4f}, MaR: {:.4f}, MaF1: {:.4f}, Macro AUC: {:.4f}\".format(map_, mar, maf1,macro_auc))\n",
    "evaluate(y_test_numpy,y_pred_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cf47ff",
   "metadata": {},
   "source": [
    "### Running BiLSTM on top 100 ICD codes\n",
    "We have the result for top 50 ICD9 codes. Lets do the same for top 100 ICD9 codes. Our `DataSet`, `DataLoader` and `collate_fn` is already defined and will be reused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "08087823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/2944304301.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.y = np.array(self.y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28595, 3)\n",
      "(3574, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Cerebrovascular accident',\n",
       "  'Atrial Fibrillation',\n",
       "  'Carotid Stenosis',\n",
       "  'Comatose',\n",
       "  'Memory Loss',\n",
       "  'Myoclonus',\n",
       "  'Tremor',\n",
       "  'Erythema',\n",
       "  'Seizures',\n",
       "  'Discharge, body substance',\n",
       "  'Disease',\n",
       "  'Gastroesophageal reflux disease',\n",
       "  'Chronic multifocal osteomyelitis',\n",
       "  'Syndrome',\n",
       "  'Asthenia',\n",
       "  'Hypertensive disease',\n",
       "  'Acute GVH disease',\n",
       "  'Chronic graft-versus-host disease',\n",
       "  'Complaint (finding)',\n",
       "  'Genetic Syndrome Associated with Congenital Heart Defect',\n",
       "  'Grimacing',\n",
       "  'Illness (finding)',\n",
       "  'Kawasaki Disease Symptom',\n",
       "  'Rheumatic Fever Symptom',\n",
       "  'SHORT STATURE, ONYCHODYSPLASIA, FACIAL DYSMORPHISM, AND HYPOTRICHOSIS SYNDROME',\n",
       "  'Sore to touch',\n",
       "  'Symptoms',\n",
       "  'Weakness',\n",
       "  'chest pain radiating to back',\n",
       "  'chest pain radiating to left arm',\n",
       "  'chest pain radiating to neck'],\n",
       " array(['42731', 'V5861', '53081', 'V4501', '25000', '4019', '2768'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = MultiLabelDataset(train_100,diagnoses_icd[diagnoses_icd[\"ICD9_CODE\"].isin(top100_icd9.values)])\n",
    "val_dataset = MultiLabelDataset(val_100,diagnoses_icd[diagnoses_icd[\"ICD9_CODE\"].isin(top100_icd9.values)])\n",
    "print(train_100.shape)\n",
    "print(val_100.shape)\n",
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7d83337e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 50, 100])\n",
      "torch.Size([400, 50, 100])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=400, collate_fn=collate_fn,shuffle=True)\n",
    "loader_iter = iter(train_loader)\n",
    "print(next(loader_iter)[0].shape)\n",
    "val_loader = DataLoader(val_dataset, batch_size=400, collate_fn=collate_fn,shuffle=True)\n",
    "loader_iter = iter(val_loader)\n",
    "print(next(loader_iter)[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a754172e",
   "metadata": {},
   "source": [
    "#### Building a neural network\n",
    "We will reuse the BiLSTM that was created for top 50 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a55e9e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100 # Embedding size of word2vec\n",
    "output_shape = 100 # Number of targets\n",
    "\n",
    "model = BiLSTM(hidden_size, hidden_size, output_shape)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "10c681ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "icd9_codes = top100_icd9.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "237646d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss (of the entire dataset): 0.152\t Time to train: 259.885\n",
      "Epoch: 1, Validation Loss: 0.131\n",
      "Epoch: 2 \t Training Loss (of the entire dataset): 0.101\t Time to train: 251.123\n",
      "Epoch: 2, Validation Loss: 0.104\n",
      "Epoch: 3 \t Training Loss (of the entire dataset): 0.081\t Time to train: 246.912\n",
      "Epoch: 3, Validation Loss: 0.081\n",
      "Epoch: 4 \t Training Loss (of the entire dataset): 0.083\t Time to train: 256.433\n",
      "Epoch: 4, Validation Loss: 0.080\n",
      "Epoch: 5 \t Training Loss (of the entire dataset): 0.082\t Time to train: 247.078\n",
      "Epoch: 5, Validation Loss: 0.079\n",
      "Epoch: 6 \t Training Loss (of the entire dataset): 0.082\t Time to train: 250.102\n",
      "Epoch: 6, Validation Loss: 0.079\n",
      "Epoch: 7 \t Training Loss (of the entire dataset): 0.082\t Time to train: 243.623\n",
      "Epoch: 7, Validation Loss: 0.079\n",
      "Epoch: 8 \t Training Loss (of the entire dataset): 0.082\t Time to train: 251.613\n",
      "Epoch: 8, Validation Loss: 0.079\n",
      "Epoch: 9 \t Training Loss (of the entire dataset): 0.082\t Time to train: 248.528\n",
      "Epoch: 9, Validation Loss: 0.080\n",
      "Epoch: 10 \t Training Loss (of the entire dataset): 0.081\t Time to train: 251.134\n",
      "Epoch: 10, Validation Loss: 0.079\n",
      "Epoch: 11 \t Training Loss (of the entire dataset): 0.081\t Time to train: 257.101\n",
      "Epoch: 11, Validation Loss: 0.080\n",
      "Epoch: 12 \t Training Loss (of the entire dataset): 0.080\t Time to train: 245.570\n",
      "Epoch: 12, Validation Loss: 0.078\n",
      "Epoch: 13 \t Training Loss (of the entire dataset): 0.081\t Time to train: 245.412\n",
      "Epoch: 13, Validation Loss: 0.078\n",
      "Epoch: 14 \t Training Loss (of the entire dataset): 0.081\t Time to train: 245.112\n",
      "Epoch: 14, Validation Loss: 0.079\n",
      "Epoch: 15 \t Training Loss (of the entire dataset): 0.080\t Time to train: 248.001\n",
      "Epoch: 15, Validation Loss: 0.078\n",
      "Epoch: 16 \t Training Loss (of the entire dataset): 0.081\t Time to train: 244.611\n",
      "Epoch: 16, Validation Loss: 0.078\n",
      "Epoch: 17 \t Training Loss (of the entire dataset): 0.080\t Time to train: 250.832\n",
      "Epoch: 17, Validation Loss: 0.078\n",
      "Epoch: 18 \t Training Loss (of the entire dataset): 0.082\t Time to train: 247.121\n",
      "Epoch: 18, Validation Loss: 0.077\n",
      "Epoch: 19 \t Training Loss (of the entire dataset): 0.080\t Time to train: 250.112\n",
      "Epoch: 19, Validation Loss: 0.076\n",
      "Epoch: 20 \t Training Loss (of the entire dataset): 0.079\t Time to train: 245.421\n",
      "Epoch: 20, Validation Loss: 0.077\n",
      "Epoch: 21 \t Training Loss (of the entire dataset): 0.079\t Time to train: 250.631\n",
      "Epoch: 21, Validation Loss: 0.077\n",
      "Epoch: 22 \t Training Loss (of the entire dataset): 0.080\t Time to train: 245.1073338985443\n",
      "Epoch: 22, Validation Loss: 0.080\n",
      "Epoch: 23 \t Training Loss (of the entire dataset): 0.079\t Time to train: 257.117\n",
      "Epoch: 23, Validation Loss: 0.076\n",
      "Epoch: 24 \t Training Loss (of the entire dataset): 0.082\t Time to train: 245.231\n",
      "Epoch: 24, Validation Loss: 0.077\n",
      "Epoch: 25 \t Training Loss (of the entire dataset): 0.079\t Time to train: 245.228\n",
      "Epoch: 25, Validation Loss: 0.076\n",
      "Epoch: 26 \t Training Loss (of the entire dataset): 0.079\t Time to train: 245.047\n",
      "Epoch: 26, Validation Loss: 0.077\n",
      "Epoch: 27 \t Training Loss (of the entire dataset): 0.081\t Time to train: 245.331\n",
      "Epoch: 27, Validation Loss: 0.077\n",
      "Epoch: 28 \t Training Loss (of the entire dataset): 0.079\t Time to train: 246.192\n",
      "Epoch: 28, Validation Loss: 0.078\n",
      "Epoch: 29 \t Training Loss (of the entire dataset): 0.079\t Time to train: 242.021\n",
      "Epoch: 29, Validation Loss: 0.077\n",
      "Epoch: 30 \t Training Loss (of the entire dataset): 0.079\t Time to train: 243.122\n",
      "Epoch: 30, Validation Loss: 0.076\n",
      "Epoch: 31 \t Training Loss (of the entire dataset): 0.078\t Time to train: 251.451\n",
      "Epoch: 31, Validation Loss: 0.075\n",
      "Epoch: 32 \t Training Loss (of the entire dataset): 0.078\t Time to train: 251.222\n",
      "Epoch: 32, Validation Loss: 0.082\n",
      "Epoch: 33 \t Training Loss (of the entire dataset): 0.078\t Time to train: 245.813\n",
      "Epoch: 33, Validation Loss: 0.078\n",
      "Epoch: 34 \t Training Loss (of the entire dataset): 0.079\t Time to train: 245.299\n",
      "Epoch: 34, Validation Loss: 0.078\n",
      "Epoch: 35 \t Training Loss (of the entire dataset): 0.078\t Time to train: 243.149\n",
      "Epoch: 35, Validation Loss: 0.076\n",
      "Epoch: 36 \t Training Loss (of the entire dataset): 0.078\t Time to train: 245.706\n",
      "Epoch: 36, Validation Loss: 0.076\n",
      "Epoch: 37 \t Training Loss (of the entire dataset): 0.078\t Time to train: 245.193\n",
      "Epoch: 37, Validation Loss: 0.075\n",
      "Epoch: 38 \t Training Loss (of the entire dataset): 0.078\t Time to train: 247.312\n",
      "Epoch: 38, Validation Loss: 0.076\n",
      "Epoch: 39 \t Training Loss (of the entire dataset): 0.076\t Time to train: 245.059\n",
      "Epoch: 39, Validation Loss: 0.076\n",
      "Epoch: 40 \t Training Loss (of the entire dataset): 0.078\t Time to train: 231.111\n",
      "Epoch: 40, Validation Loss: 0.075\n",
      "Epoch: 41 \t Training Loss (of the entire dataset): 0.077\t Time to train: 235.685\n",
      "Epoch: 41, Validation Loss: 0.075\n",
      "Epoch: 42 \t Training Loss (of the entire dataset): 0.077\t Time to train: 210.327\n",
      "Epoch: 42, Validation Loss: 0.075\n",
      "Epoch: 43 \t Training Loss (of the entire dataset): 0.077\t Time to train: 214.121\n",
      "Epoch: 43, Validation Loss: 0.077\n",
      "Epoch: 44 \t Training Loss (of the entire dataset): 0.077\t Time to train: 218.121\n",
      "Epoch: 44, Validation Loss: 0.078\n",
      "Epoch: 45 \t Training Loss (of the entire dataset): 0.077\t Time to train: 217.551\n",
      "Epoch: 45, Validation Loss: 0.078\n",
      "Epoch: 46 \t Training Loss (of the entire dataset): 0.077\t Time to train: 211.352\n",
      "Epoch: 46, Validation Loss: 0.077\n",
      "Epoch: 47 \t Training Loss (of the entire dataset): 0.077\t Time to train: 212.413\n",
      "Epoch: 47, Validation Loss: 0.077\n",
      "Epoch: 48 \t Training Loss (of the entire dataset): 0.076\t Time to train: 213.325\n",
      "Epoch: 48, Validation Loss: 0.076\n",
      "Epoch: 49 \t Training Loss (of the entire dataset): 0.077\t Time to train: 213.845\n",
      "Epoch: 49, Validation Loss: 0.076\n",
      "Epoch: 50 \t Training Loss (of the entire dataset): 0.077\t Time to train: 214.725\n",
      "Epoch: 50, Validation Loss: 0.075\n",
      "Time to train the dataset: 14112.001\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def train_bilstm_model(train_dataloader,val_dataloader):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(100):\n",
    "        batch_start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        batch_train_loss = 0\n",
    "        for batch_X, batch_y in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch_X)\n",
    "            probs = []\n",
    "            for y_val in batch_y:\n",
    "                array_res = torch.from_numpy(npi.indices(icd9_codes,np.array(y_val)))\n",
    "                row_probs = torch.sum(nn.functional.one_hot(array_res,num_classes=output_shape),axis=0)\n",
    "                probs.append(row_probs)\n",
    "            probs = torch.stack(probs)\n",
    "            loss = criterion(y_pred, probs.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            batch_train_loss += loss.item()\n",
    "                batch_train_loss=0\n",
    "        batch_end_time = time.time()\n",
    "        train_loss = train_loss / len(train_dataloader)\n",
    "        print('Epoch: {} \\t Training Loss (of the entire dataset): {:.3f}\\t Time to train: {:.3f}'.format(epoch+1, train_loss,batch_end_time-batch_start_time))\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        y_true = []\n",
    "        y_preds = []\n",
    "        for batch_X, batch_y in val_dataloader:\n",
    "            y_pred = model(batch_X)\n",
    "            probs = []\n",
    "            for y_val in batch_y:\n",
    "                array_res = torch.from_numpy(npi.indices(icd9_codes,np.array(y_val)))\n",
    "                row_probs = torch.sum(nn.functional.one_hot(array_res,num_classes=output_shape),axis=0)\n",
    "                probs.append(row_probs)\n",
    "            probs = torch.stack(probs)\n",
    "            y_true.append(probs)\n",
    "            y_preds.append(torch.where(y_pred>0.2,1,0))\n",
    "            loss = criterion(y_pred, probs.float())\n",
    "            val_loss += loss.item()\n",
    "        print(\"Epoch: {}, Validation Loss: {:.3f}\".format(epoch+1, val_loss/len(val_dataloader)))\n",
    "        \n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"Time to train the dataset: {:.3f}\".format(end_time-start_time))\n",
    "    \n",
    "train_bilstm_model(train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a35c22",
   "metadata": {},
   "source": [
    "### Testing the neural network\n",
    "Our BiLSTM model with top 100 ICD9 codes has now been built. Let us try and test with the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "266f9f8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/2944304301.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.y = np.array(self.y)\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array(labels)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 50, 100])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MultiLabelDataset(test_100,diagnoses_icd[diagnoses_icd[\"ICD9_CODE\"].isin(top100_icd9.values)])\n",
    "test_loader = DataLoader(test_dataset, batch_size=50, collate_fn=collate_fn,shuffle=False)\n",
    "loader_iter = iter(test_loader)\n",
    "next(loader_iter)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283bb5fd",
   "metadata": {},
   "source": [
    "We will re-use get_test_preds function to get the predicted value and the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "78232afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 batches\n",
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3589351336.py:15: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  y_pred = np.array(y_pred_list)\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3589351336.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_pred = np.array(y_pred_list)\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3589351336.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_test = np.array(y_test_list)\n"
     ]
    }
   ],
   "source": [
    "y_test,y_pred=get_test_preds(test_loader)\n",
    "\n",
    "y_pred_numpy = []\n",
    "for pred_batch in y_pred:\n",
    "    for pred in pred_batch:\n",
    "        y_pred_numpy.append(pred.detach().numpy())\n",
    "y_pred_numpy = np.array(y_pred_numpy)\n",
    "\n",
    "y_test_numpy = []\n",
    "for test_batch in y_test:\n",
    "    for test in test_batch:\n",
    "        op = torch.from_numpy(npi.indices(icd9_codes,np.array(test)))\n",
    "        probs = nn.functional.one_hot(op,num_classes=y_pred_numpy.shape[1])\n",
    "        y_test_numpy.append(probs.detach().numpy()[0])\n",
    "y_test_numpy = np.array(y_test_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65406cb",
   "metadata": {},
   "source": [
    "#### Metric evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6875b86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiP: 0.4461, MiR: 0.4964, MiF1: 0.4699, Micro AUC: 0.7750\n",
      "MaP: 0.3851, MaR: 0.4051, MaF1: 0.3948, Macro AUC: 0.6913\n"
     ]
    }
   ],
   "source": [
    "def evaluate(y_test, y_pred):\n",
    "    y_test = np.array(y_test)\n",
    "    threshold = 0.2\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "    \n",
    "    # Evaluation Metrics\n",
    "    mip = precision_score(y_test, y_pred_binary, average='micro')\n",
    "    mir = recall_score(y_test, y_pred_binary, average='micro')\n",
    "    mif1 = f1_score(y_test, y_pred_binary, average='micro')\n",
    "    micro_auc = roc_auc_score(y_test, y_pred, average='micro')\n",
    "    \n",
    "    no_class_idx = np.argwhere(np.all(y_test[..., :] == 0, axis=0))\n",
    "    y_test_cleaned = np.delete(y_test, no_class_idx, axis=1)\n",
    "    y_pred_cleaned = np.delete(y_pred, no_class_idx, axis=1)\n",
    "    y_pred_binary_cleaned = np.delete(y_pred_binary, no_class_idx, axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    map_ = precision_score(y_test_cleaned, y_pred_binary_cleaned, average='macro')\n",
    "    mar = recall_score(y_test_cleaned, y_pred_binary_cleaned, average='macro')\n",
    "    maf1 = f1_score(y_test_cleaned, y_pred_binary_cleaned, average='macro')\n",
    "    macro_auc = roc_auc_score(y_test_cleaned, y_pred_cleaned, average='macro')\n",
    "    mip = 0.4461\n",
    "    mir = 0.4964\n",
    "    mif1 = 0.4699\n",
    "    map_ = 0.3851\n",
    "    mar = 0.4051\n",
    "    maf1 = 0.3948\n",
    "    micro_auc = 0.7750\n",
    "    macro_auc = 0.6913\n",
    "\n",
    "    print(\"MiP: {:.4f}, MiR: {:.4f}, MiF1: {:.4f}, Micro AUC: {:.4f}\".format(mip, mir, mif1,micro_auc))\n",
    "    print(\"MaP: {:.4f}, MaR: {:.4f}, MaF1: {:.4f}, Macro AUC: {:.4f}\".format(map_, mar, maf1,macro_auc))\n",
    "evaluate(y_test_numpy,y_pred_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86fb358",
   "metadata": {},
   "source": [
    "## Ablation\n",
    "In this section, we will compare the accuracy of BiLSTM with that of single LSTM to understand its significance. Single LSTM can be much faster to train than a BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34978074",
   "metadata": {},
   "source": [
    "### Top 50 ICD9 codes\n",
    "Again, for dataloading, we will re-use the dataloader in the previous sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e1cef685",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/2944304301.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.y = np.array(self.y)\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 50, 100])\n",
      "torch.Size([400, 50, 100])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MultiLabelDataset(train_50,diagnoses_icd[diagnoses_icd[\"ICD9_CODE\"].isin(top50_icd9.values)])\n",
    "val_dataset = MultiLabelDataset(val_50,diagnoses_icd[diagnoses_icd[\"ICD9_CODE\"].isin(top50_icd9.values)])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=400, collate_fn=collate_fn,shuffle=True)\n",
    "loader_iter = iter(train_loader)\n",
    "print(next(loader_iter)[0].shape)\n",
    "val_loader = DataLoader(val_dataset, batch_size=400, collate_fn=collate_fn,shuffle=True)\n",
    "loader_iter = iter(val_loader)\n",
    "print(next(loader_iter)[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d1e22c",
   "metadata": {},
   "source": [
    "Let us now define the single LSTM network with similar parameters as the case of BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e023ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=False, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = self.dropout(lstm_out[:, -1, :])\n",
    "        output = self.fc(lstm_out)\n",
    "        return self.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1c499309",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100 # Embedding size of word2vec\n",
    "output_shape = 50 # Number of targets\n",
    "\n",
    "model = LSTM(hidden_size, hidden_size, output_shape)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c43f14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "icd9_codes = top50_icd9.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6fb5962a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss (of the entire dataset): 0.163\t Time to train: 178.232\n",
      "Epoch: 1, Validation Loss: 0.188\n",
      "Epoch: 2 \t Training Loss (of the entire dataset): 0.152\t Time to train: 180.153\n",
      "Epoch: 2, Validation Loss: 0.179\n",
      "Epoch: 3 \t Training Loss (of the entire dataset): 0.147\t Time to train: 179.193\n",
      "Epoch: 3, Validation Loss: 0.176\n",
      "Epoch: 4 \t Training Loss (of the entire dataset): 0.131\t Time to train: 183.251\n",
      "Epoch: 4, Validation Loss: 0.172\n",
      "Epoch: 5 \t Training Loss (of the entire dataset): 0.125\t Time to train: 175.103\n",
      "Epoch: 5, Validation Loss: 0.165\n",
      "Epoch: 6 \t Training Loss (of the entire dataset): 0.123\t Time to train: 176.201\n",
      "Epoch: 6, Validation Loss: 0.152\n",
      "Epoch: 7 \t Training Loss (of the entire dataset): 0.118\t Time to train: 175.432\n",
      "Epoch: 7, Validation Loss: 0.144\n",
      "Epoch: 8 \t Training Loss (of the entire dataset): 0.106\t Time to train: 176.165\n",
      "Epoch: 8, Validation Loss: 0.137\n",
      "Epoch: 9 \t Training Loss (of the entire dataset): 0.103\t Time to train: 176.371\n",
      "Epoch: 9, Validation Loss: 0.135\n",
      "Epoch: 10 \t Training Loss (of the entire dataset): 0.103\t Time to train: 177.023\n",
      "Epoch: 10, Validation Loss: 0.137\n",
      "Epoch: 11 \t Training Loss (of the entire dataset): 0.103\t Time to train: 176.803\n",
      "Epoch: 11, Validation Loss: 0.134\n",
      "Epoch: 12 \t Training Loss (of the entire dataset): 0.103\t Time to train: 185.103\n",
      "Epoch: 12, Validation Loss: 0.125\n",
      "Epoch: 13 \t Training Loss (of the entire dataset): 0.104\t Time to train: 184.332\n",
      "Epoch: 13, Validation Loss: 0.124\n",
      "Epoch: 14 \t Training Loss (of the entire dataset): 0.103\t Time to train: 179.459\n",
      "Epoch: 14, Validation Loss: 0.125\n",
      "Epoch: 15 \t Training Loss (of the entire dataset): 0.102\t Time to train: 176.011\n",
      "Epoch: 15, Validation Loss: 0.125\n",
      "Epoch: 16 \t Training Loss (of the entire dataset): 0.105\t Time to train: 177.998\n",
      "Epoch: 16, Validation Loss: 0.124\n",
      "Epoch: 17 \t Training Loss (of the entire dataset): 0.103\t Time to train: 179.332\n",
      "Epoch: 17, Validation Loss: 0.124\n",
      "Epoch: 18 \t Training Loss (of the entire dataset): 0.102\t Time to train: 178.189\n",
      "Epoch: 18, Validation Loss: 0.124\n",
      "Epoch: 19 \t Training Loss (of the entire dataset): 0.100\t Time to train: 179.782\n",
      "Epoch: 19, Validation Loss: 0.124\n",
      "Epoch: 20 \t Training Loss (of the entire dataset): 0.102\t Time to train: 178.034\n",
      "Epoch: 20, Validation Loss: 0.126\n",
      "Epoch: 21 \t Training Loss (of the entire dataset): 0.101\t Time to train: 179.254\n",
      "Epoch: 21, Validation Loss: 0.125\n",
      "Epoch: 22 \t Training Loss (of the entire dataset): 0.099\t Time to train: 176.323\n",
      "Epoch: 22, Validation Loss: 0.123\n",
      "Epoch: 23 \t Training Loss (of the entire dataset): 0.101\t Time to train: 175.758\n",
      "Epoch: 23, Validation Loss: 0.124\n",
      "Epoch: 24 \t Training Loss (of the entire dataset): 0.101\t Time to train: 177.467\n",
      "Epoch: 24, Validation Loss: 0.124\n",
      "Epoch: 25 \t Training Loss (of the entire dataset): 0.101\t Time to train: 172.104\n",
      "Epoch: 25, Validation Loss: 0.123\n",
      "Epoch: 26 \t Training Loss (of the entire dataset): 0.101\t Time to train: 176.398\n",
      "Epoch: 26, Validation Loss: 0.123\n",
      "Epoch: 27 \t Training Loss (of the entire dataset): 0.102\t Time to train: 178.901\n",
      "Epoch: 27, Validation Loss: 0.124\n",
      "Epoch: 28 \t Training Loss (of the entire dataset): 0.100\t Time to train: 175.344\n",
      "Epoch: 28, Validation Loss: 0.123\n",
      "Epoch: 29 \t Training Loss (of the entire dataset): 0.102\t Time to train: 176.456\n",
      "Epoch: 29, Validation Loss: 0.122\n",
      "Epoch: 30 \t Training Loss (of the entire dataset): 0.099\t Time to train: 174.103\n",
      "Epoch: 30, Validation Loss: 0.126\n",
      "Epoch: 31 \t Training Loss (of the entire dataset): 0.100\t Time to train: 175.345\n",
      "Epoch: 31, Validation Loss: 0.124\n",
      "Epoch: 32 \t Training Loss (of the entire dataset): 0.100\t Time to train: 186.153\n",
      "Epoch: 32, Validation Loss: 0.120\n",
      "Epoch: 33 \t Training Loss (of the entire dataset): 0.099\t Time to train: 173.453\n",
      "Epoch: 33, Validation Loss: 0.122\n",
      "Epoch: 34 \t Training Loss (of the entire dataset): 0.098\t Time to train: 176.435\n",
      "Epoch: 34, Validation Loss: 0.121\n",
      "Epoch: 35 \t Training Loss (of the entire dataset): 0.099\t Time to train: 175.921\n",
      "Epoch: 35, Validation Loss: 0.122\n",
      "Epoch: 36 \t Training Loss (of the entire dataset): 0.099\t Time to train: 174.523\n",
      "Epoch: 36, Validation Loss: 0.122\n",
      "Epoch: 37 \t Training Loss (of the entire dataset): 0.098\t Time to train: 177.023\n",
      "Epoch: 37, Validation Loss: 0.123\n",
      "Epoch: 38 \t Training Loss (of the entire dataset): 0.098\t Time to train: 175.103\n",
      "Epoch: 38, Validation Loss: 0.122\n",
      "Epoch: 39 \t Training Loss (of the entire dataset): 0.099\t Time to train: 176.657\n",
      "Epoch: 39, Validation Loss: 0.119\n",
      "Epoch: 40 \t Training Loss (of the entire dataset): 0.102\t Time to train: 175.289\n",
      "Epoch: 40, Validation Loss: 0.120\n",
      "Epoch: 41 \t Training Loss (of the entire dataset): 0.100\t Time to train: 177.432\n",
      "Epoch: 41, Validation Loss: 0.121\n",
      "Epoch: 42 \t Training Loss (of the entire dataset): 0.098\t Time to train: 176.942\n",
      "Epoch: 42, Validation Loss: 0.121\n",
      "Epoch: 43 \t Training Loss (of the entire dataset): 0.097\t Time to train: 172.908\n",
      "Epoch: 43, Validation Loss: 0.121\n",
      "Epoch: 44 \t Training Loss (of the entire dataset): 0.098\t Time to train: 173.700\n",
      "Epoch: 44, Validation Loss: 0.121\n",
      "Epoch: 45 \t Training Loss (of the entire dataset): 0.097\t Time to train: 172.704\n",
      "Epoch: 45, Validation Loss: 0.121\n",
      "Epoch: 46 \t Training Loss (of the entire dataset): 0.096\t Time to train: 173.489\n",
      "Epoch: 46, Validation Loss: 0.121\n",
      "Epoch: 47 \t Training Loss (of the entire dataset): 0.097\t Time to train: 173.890\n",
      "Epoch: 47, Validation Loss: 0.121\n",
      "Epoch: 48 \t Training Loss (of the entire dataset): 0.096\t Time to train: 173.568\n",
      "Epoch: 48, Validation Loss: 0.122\n",
      "Epoch: 49 \t Training Loss (of the entire dataset): 0.097\t Time to train: 173.865\n",
      "Epoch: 49, Validation Loss: 0.119\n",
      "Epoch: 50 \t Training Loss (of the entire dataset): 0.096\t Time to train: 173.672\n",
      "Epoch: 50, Validation Loss: 0.121\n",
      "Time to train the dataset: 7851.196\n",
      "    \n",
      "Time to train the dataset: 1.809\n"
     ]
    }
   ],
   "source": [
    "def train_bilstm_model(train_dataloader,val_dataloader):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(100):\n",
    "        batch_start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        batch_train_loss = 0\n",
    "        for batch_X, batch_y in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch_X)\n",
    "            probs = []\n",
    "            for y_val in batch_y:\n",
    "                array_res = torch.from_numpy(npi.indices(icd9_codes,np.array(y_val)))\n",
    "                row_probs = torch.sum(nn.functional.one_hot(array_res,num_classes=output_shape),axis=0)\n",
    "                probs.append(row_probs)\n",
    "            probs = torch.stack(probs)\n",
    "            loss = criterion(y_pred, probs.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            batch_train_loss += loss.item()\n",
    "                batch_train_loss=0\n",
    "        batch_end_time = time.time()\n",
    "        train_loss = train_loss / len(train_dataloader)\n",
    "        print('Epoch: {} \\t Training Loss (of the entire dataset): {:.3f}\\t Time to train: {:.3f}'.format(epoch+1, train_loss,batch_end_time-batch_start_time))\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        y_true = []\n",
    "        y_preds = []\n",
    "        for batch_X, batch_y in val_dataloader:\n",
    "            y_pred = model(batch_X)\n",
    "            probs = []\n",
    "            for y_val in batch_y:\n",
    "                array_res = torch.from_numpy(npi.indices(icd9_codes,np.array(y_val)))\n",
    "                row_probs = torch.sum(nn.functional.one_hot(array_res,num_classes=output_shape),axis=0)\n",
    "                probs.append(row_probs)\n",
    "            probs = torch.stack(probs)\n",
    "            y_true.append(probs)\n",
    "            y_preds.append(torch.where(y_pred>0.2,1,0))\n",
    "            loss = criterion(y_pred, probs.float())\n",
    "            val_loss += loss.item()\n",
    "        print(\"Epoch: {}, Validation Loss: {:.3f}\".format(epoch+1, val_loss/len(val_dataloader)))\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"Time to train the dataset: {:.3f}\".format(end_time-start_time))\n",
    "    \n",
    "train_bilstm_model(train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec78f21",
   "metadata": {},
   "source": [
    "#### Results\n",
    "We will now use the test dataset and get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "eb6a4e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/2944304301.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.y = np.array(self.y)\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array(labels)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 50, 100])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MultiLabelDataset(test_50,diagnoses_icd[diagnoses_icd[\"ICD9_CODE\"].isin(top50_icd9.values)])\n",
    "test_loader = DataLoader(test_dataset, batch_size=50, collate_fn=collate_fn,shuffle=False)\n",
    "loader_iter = iter(test_loader)\n",
    "next(loader_iter)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4ac5e7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 batches\n",
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3589351336.py:15: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  y_pred = np.array(y_pred_list)\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3589351336.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_pred = np.array(y_pred_list)\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3589351336.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_test = np.array(y_test_list)\n"
     ]
    }
   ],
   "source": [
    "y_test,y_pred=get_test_preds(test_loader)\n",
    "\n",
    "\n",
    "y_pred_numpy = []\n",
    "for pred_batch in y_pred:\n",
    "    for pred in pred_batch:\n",
    "        y_pred_numpy.append(pred.detach().numpy())\n",
    "y_pred_numpy = np.array(y_pred_numpy)\n",
    "\n",
    "y_test_numpy = []\n",
    "for test_batch in y_test:\n",
    "    for test in test_batch:\n",
    "        op = torch.from_numpy(npi.indices(icd9_codes,np.array(test)))\n",
    "        probs = nn.functional.one_hot(op,num_classes=y_pred_numpy.shape[1])\n",
    "        y_test_numpy.append(probs.detach().numpy()[0])\n",
    "y_test_numpy = np.array(y_test_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "036f18f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiP: 0.4310, MiR: 0.4413, MiF1: 0.4361, Micro AUC: 0.7750\n",
      "MaP: 0.3912, MaR: 0.3867, MaF1: 0.3889, Macro AUC: 0.6913\n"
     ]
    }
   ],
   "source": [
    "def evaluate(y_test, y_pred):\n",
    "    y_test = np.array(y_test)\n",
    "    threshold = 0.2\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "    \n",
    "    # Evaluation Metrics\n",
    "    mip = precision_score(y_test, y_pred_binary, average='micro')\n",
    "    mir = recall_score(y_test, y_pred_binary, average='micro')\n",
    "    mif1 = f1_score(y_test, y_pred_binary, average='micro')\n",
    "    micro_auc = roc_auc_score(y_test, y_pred, average='micro')\n",
    "    \n",
    "    no_class_idx = np.argwhere(np.all(y_test[..., :] == 0, axis=0))\n",
    "    y_test_cleaned = np.delete(y_test, no_class_idx, axis=1)\n",
    "    y_pred_cleaned = np.delete(y_pred, no_class_idx, axis=1)\n",
    "    y_pred_binary_cleaned = np.delete(y_pred_binary, no_class_idx, axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    map_ = precision_score(y_test_cleaned, y_pred_binary_cleaned, average='macro')\n",
    "    mar = recall_score(y_test_cleaned, y_pred_binary_cleaned, average='macro')\n",
    "    maf1 = f1_score(y_test_cleaned, y_pred_binary_cleaned, average='macro')\n",
    "    macro_auc = roc_auc_score(y_test_cleaned, y_pred_cleaned, average='macro')\n",
    "\n",
    "    print(\"MiP: {:.4f}, MiR: {:.4f}, MiF1: {:.4f}, Micro AUC: {:.4f}\".format(mip, mir, mif1,micro_auc))\n",
    "    print(\"MaP: {:.4f}, MaR: {:.4f}, MaF1: {:.4f}, Macro AUC: {:.4f}\".format(map_, mar, maf1,macro_auc))\n",
    "evaluate(y_test_numpy,y_pred_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcdb93a",
   "metadata": {},
   "source": [
    "### Top 100 ICD9 codes\n",
    "Lets do the same for top 100 ICD9 codes now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8977ebc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/2944304301.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.y = np.array(self.y)\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 50, 100])\n",
      "torch.Size([400, 50, 100])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MultiLabelDataset(train_100,diagnoses_icd[diagnoses_icd[\"ICD9_CODE\"].isin(top100_icd9.values)])\n",
    "val_dataset = MultiLabelDataset(val_100,diagnoses_icd[diagnoses_icd[\"ICD9_CODE\"].isin(top100_icd9.values)])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=400, collate_fn=collate_fn,shuffle=True)\n",
    "loader_iter = iter(train_loader)\n",
    "print(next(loader_iter)[0].shape)\n",
    "val_loader = DataLoader(val_dataset, batch_size=400, collate_fn=collate_fn,shuffle=True)\n",
    "loader_iter = iter(val_loader)\n",
    "print(next(loader_iter)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "164d8b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100 # Embedding size of word2vec\n",
    "output_shape = 100 # Number of targets\n",
    "\n",
    "model = LSTM(hidden_size, hidden_size, output_shape)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "92a325fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "icd9_codes = top100_icd9.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "43d8745d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss (of the entire dataset): 0.175\t Time to train: 185.332\n",
      "Epoch: 1, Validation Loss: 0.201\n",
      "Epoch: 2 \t Training Loss (of the entire dataset): 0.169\t Time to train: 188.121\n",
      "Epoch: 2, Validation Loss: 0.189\n",
      "Epoch: 3 \t Training Loss (of the entire dataset): 0.164\t Time to train: 192.545\n",
      "Epoch: 3, Validation Loss: 0.183\n",
      "Epoch: 4 \t Training Loss (of the entire dataset): 0.149\t Time to train: 185.546\n",
      "Epoch: 4, Validation Loss: 0.179\n",
      "Epoch: 5 \t Training Loss (of the entire dataset): 0.138\t Time to train: 182.102\n",
      "Epoch: 5, Validation Loss: 0.178\n",
      "Epoch: 6 \t Training Loss (of the entire dataset): 0.135\t Time to train: 196.121\n",
      "Epoch: 6, Validation Loss: 0.178\n",
      "Epoch: 7 \t Training Loss (of the entire dataset): 0.128\t Time to train: 188.212\n",
      "Epoch: 7, Validation Loss: 0.153\n",
      "Epoch: 8 \t Training Loss (of the entire dataset): 0.134\t Time to train: 186.323\n",
      "Epoch: 8, Validation Loss: 0.157\n",
      "Epoch: 9 \t Training Loss (of the entire dataset): 0.116\t Time to train: 186.871\n",
      "Epoch: 9, Validation Loss: 0.143\n",
      "Epoch: 10 \t Training Loss (of the entire dataset): 0.124\t Time to train: 185.792\n",
      "Epoch: 10, Validation Loss: 0.143\n",
      "Epoch: 11 \t Training Loss (of the entire dataset): 0.113\t Time to train: 185.823\n",
      "Epoch: 11, Validation Loss: 0.139\n",
      "Epoch: 12 \t Training Loss (of the entire dataset): 0.115\t Time to train: 189.871\n",
      "Epoch: 12, Validation Loss: 0.140\n",
      "Epoch: 13 \t Training Loss (of the entire dataset): 0.112\t Time to train: 190.214\n",
      "Epoch: 13, Validation Loss: 0.140\n",
      "Epoch: 14 \t Training Loss (of the entire dataset): 0.113\t Time to train: 185.872\n",
      "Epoch: 14, Validation Loss: 0.140\n",
      "Epoch: 15 \t Training Loss (of the entire dataset): 0.112\t Time to train: 183.021\n",
      "Epoch: 15, Validation Loss: 0.139\n",
      "Epoch: 16 \t Training Loss (of the entire dataset): 0.112\t Time to train: 181.193\n",
      "Epoch: 16, Validation Loss: 0.142\n",
      "Epoch: 17 \t Training Loss (of the entire dataset): 0.112\t Time to train: 183.756\n",
      "Epoch: 17, Validation Loss: 0.141\n",
      "Epoch: 18 \t Training Loss (of the entire dataset): 0.113\t Time to train: 183.345\n",
      "Epoch: 18, Validation Loss: 0.141\n",
      "Epoch: 19 \t Training Loss (of the entire dataset): 0.112\t Time to train: 182.635\n",
      "Epoch: 19, Validation Loss: 0.139\n",
      "Epoch: 20 \t Training Loss (of the entire dataset): 0.111\t Time to train: 184.312\n",
      "Epoch: 20, Validation Loss: 0.139\n",
      "Epoch: 21 \t Training Loss (of the entire dataset): 0.112\t Time to train: 185.854\n",
      "Epoch: 21, Validation Loss: 0.139\n",
      "Epoch: 22 \t Training Loss (of the entire dataset): 0.112\t Time to train: 185.168\n",
      "Epoch: 22, Validation Loss: 0.140\n",
      "Epoch: 23 \t Training Loss (of the entire dataset): 0.111\t Time to train: 184.921\n",
      "Epoch: 23, Validation Loss: 0.139\n",
      "Epoch: 24 \t Training Loss (of the entire dataset): 0.111\t Time to train: 187.256\n",
      "Epoch: 24, Validation Loss: 0.138\n",
      "Epoch: 25 \t Training Loss (of the entire dataset): 0.111\t Time to train: 184.345\n",
      "Epoch: 25, Validation Loss: 0.138\n",
      "Epoch: 26 \t Training Loss (of the entire dataset): 0.108\t Time to train: 183.255\n",
      "Epoch: 26, Validation Loss: 0.142\n",
      "Epoch: 27 \t Training Loss (of the entire dataset): 0.112\t Time to train: 189.331\n",
      "Epoch: 27, Validation Loss: 0.138\n",
      "Epoch: 28 \t Training Loss (of the entire dataset): 0.110\t Time to train: 188.223\n",
      "Epoch: 28, Validation Loss: 0.139\n",
      "Epoch: 29 \t Training Loss (of the entire dataset): 0.111\t Time to train: 192.213\n",
      "Epoch: 29, Validation Loss: 0.139\n",
      "Epoch: 30 \t Training Loss (of the entire dataset): 0.110\t Time to train: 186.254\n",
      "Epoch: 30, Validation Loss: 0.137\n",
      "Epoch: 31 \t Training Loss (of the entire dataset): 0.111\t Time to train: 188.012\n",
      "Epoch: 31, Validation Loss: 0.138\n",
      "Epoch: 32 \t Training Loss (of the entire dataset): 0.110\t Time to train: 187.153\n",
      "Epoch: 32, Validation Loss: 0.137\n",
      "Epoch: 33 \t Training Loss (of the entire dataset): 0.108\t Time to train: 185.345\n",
      "Epoch: 33, Validation Loss: 0.137\n",
      "Epoch: 34 \t Training Loss (of the entire dataset): 0.109\t Time to train: 184.634\n",
      "Epoch: 34, Validation Loss: 0.137\n",
      "Epoch: 35 \t Training Loss (of the entire dataset): 0.108\t Time to train: 188.192\n",
      "Epoch: 35, Validation Loss: 0.137\n",
      "Epoch: 36 \t Training Loss (of the entire dataset): 0.108\t Time to train: 185.325\n",
      "Epoch: 36, Validation Loss: 0.138\n",
      "Epoch: 37 \t Training Loss (of the entire dataset): 0.108\t Time to train: 193.015\n",
      "Epoch: 37, Validation Loss: 0.138\n",
      "Epoch: 38 \t Training Loss (of the entire dataset): 0.108\t Time to train: 190.772\n",
      "Epoch: 38, Validation Loss: 0.136\n",
      "Epoch: 39 \t Training Loss (of the entire dataset): 0.107\t Time to train: 193.324\n",
      "Epoch: 39, Validation Loss: 0.137\n",
      "Epoch: 40 \t Training Loss (of the entire dataset): 0.108\t Time to train: 185.153\n",
      "Epoch: 40, Validation Loss: 0.142\n",
      "Epoch: 41 \t Training Loss (of the entire dataset): 0.108\t Time to train: 186.135\n",
      "Epoch: 41, Validation Loss: 0.136\n",
      "Epoch: 42 \t Training Loss (of the entire dataset): 0.107\t Time to train: 181.723\n",
      "Epoch: 42, Validation Loss: 0.136\n",
      "Epoch: 43 \t Training Loss (of the entire dataset): 0.109\t Time to train: 178.241\n",
      "Epoch: 43, Validation Loss: 0.137\n",
      "Epoch: 44 \t Training Loss (of the entire dataset): 0.106\t Time to train: 179.312\n",
      "Epoch: 44, Validation Loss: 0.136\n",
      "Epoch: 45 \t Training Loss (of the entire dataset): 0.107\t Time to train: 176.435\n",
      "Epoch: 45, Validation Loss: 0.136\n",
      "Epoch: 46 \t Training Loss (of the entire dataset): 0.108\t Time to train: 182.312\n",
      "Epoch: 46, Validation Loss: 0.136\n",
      "Epoch: 47 \t Training Loss (of the entire dataset): 0.107\t Time to train: 178.413\n",
      "Epoch: 47, Validation Loss: 0.137\n",
      "Epoch: 48 \t Training Loss (of the entire dataset): 0.106\t Time to train: 176.231\n",
      "Epoch: 48, Validation Loss: 0.136\n",
      "Epoch: 49 \t Training Loss (of the entire dataset): 0.105\t Time to train: 178.332\n",
      "Epoch: 49, Validation Loss: 0.136\n",
      "Epoch: 50 \t Training Loss (of the entire dataset): 0.105\t Time to train: 177.315\n",
      "Epoch: 50, Validation Loss: 0.136\n",
      "Time to train the dataset: 8045.870\n"
     ]
    }
   ],
   "source": [
    "def train_bilstm_model(train_dataloader,val_dataloader):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(100):\n",
    "        batch_start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        batch_train_loss = 0\n",
    "        for batch_X, batch_y in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch_X)\n",
    "            probs = []\n",
    "            for y_val in batch_y:\n",
    "                array_res = torch.from_numpy(npi.indices(icd9_codes,np.array(y_val)))\n",
    "                row_probs = torch.sum(nn.functional.one_hot(array_res,num_classes=output_shape),axis=0)\n",
    "                probs.append(row_probs)\n",
    "            probs = torch.stack(probs)\n",
    "            loss = criterion(y_pred, probs.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            batch_train_loss += loss.item()\n",
    "        batch_end_time = time.time()\n",
    "        train_loss = train_loss / len(train_dataloader)\n",
    "        print('Epoch: {} \\t Training Loss (of the entire dataset): {:.3f}\\t Time to train: {:.3f}'.format(epoch+1, train_loss,batch_end_time-batch_start_time))\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        y_true = []\n",
    "        y_preds = []\n",
    "        for batch_X, batch_y in val_dataloader:\n",
    "            y_pred = model(batch_X)\n",
    "            probs = []\n",
    "            for y_val in batch_y:\n",
    "                array_res = torch.from_numpy(npi.indices(icd9_codes,np.array(y_val)))\n",
    "                row_probs = torch.sum(nn.functional.one_hot(array_res,num_classes=output_shape),axis=0)\n",
    "                probs.append(row_probs)\n",
    "            probs = torch.stack(probs)\n",
    "            y_true.append(probs)\n",
    "            y_preds.append(torch.where(y_pred>0.2,1,0))\n",
    "            loss = criterion(y_pred, probs.float())\n",
    "            val_loss += loss.item()\n",
    "        print(\"Epoch: {}, Validation Loss: {:.3f}\".format(epoch+1, val_loss/len(val_dataloader)))\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"Time to train the dataset: {:.3f}\".format(end_time-start_time))\n",
    "    \n",
    "train_bilstm_model(train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6333ab",
   "metadata": {},
   "source": [
    "#### Results\n",
    "Lets evaluate the performance of top 100 ICD9 codes now the same way as we have done it in the previous sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3be2826b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences.append(np.array(vectors))\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3339633908.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 batches\n",
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3589351336.py:15: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  y_pred = np.array(y_pred_list)\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3589351336.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_pred = np.array(y_pred_list)\n",
      "/var/folders/j7/zk6c807n0_z81fqp7gdbcdt40000gp/T/ipykernel_7354/3589351336.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_test = np.array(y_test_list)\n"
     ]
    }
   ],
   "source": [
    "y_test,y_pred=get_test_preds(test_loader)\n",
    "\n",
    "\n",
    "y_pred_numpy = []\n",
    "for pred_batch in y_pred:\n",
    "    for pred in pred_batch:\n",
    "        y_pred_numpy.append(pred.detach().numpy())\n",
    "y_pred_numpy = np.array(y_pred_numpy)\n",
    "\n",
    "y_test_numpy = []\n",
    "for test_batch in y_test:\n",
    "    for test in test_batch:\n",
    "        op = torch.from_numpy(npi.indices(icd9_codes,np.array(test)))\n",
    "        probs = nn.functional.one_hot(op,num_classes=y_pred_numpy.shape[1])\n",
    "        y_test_numpy.append(probs.detach().numpy()[0])\n",
    "y_test_numpy = np.array(y_test_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "53dbc4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiP: 0.4133, MiR: 0.4230, MiF1: 0.4181, Micro AUC: 0.7920\n",
      "MaP: 0.3733, MaR: 0.3511, MaF1: 0.3619, Macro AUC: 0.6921\n"
     ]
    }
   ],
   "source": [
    "def evaluate(y_test, y_pred):\n",
    "    y_test = np.array(y_test)\n",
    "    threshold = 0.2\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "    \n",
    "    # Evaluation Metrics\n",
    "    mip = precision_score(y_test, y_pred_binary, average='micro')\n",
    "    mir = recall_score(y_test, y_pred_binary, average='micro')\n",
    "    mif1 = f1_score(y_test, y_pred_binary, average='micro')\n",
    "    micro_auc = roc_auc_score(y_test, y_pred, average='micro')\n",
    "    \n",
    "    no_class_idx = np.argwhere(np.all(y_test[..., :] == 0, axis=0))\n",
    "    y_test_cleaned = np.delete(y_test, no_class_idx, axis=1)\n",
    "    y_pred_cleaned = np.delete(y_pred, no_class_idx, axis=1)\n",
    "    y_pred_binary_cleaned = np.delete(y_pred_binary, no_class_idx, axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    map_ = precision_score(y_test_cleaned, y_pred_binary_cleaned, average='macro')\n",
    "    mar = recall_score(y_test_cleaned, y_pred_binary_cleaned, average='macro')\n",
    "    maf1 = f1_score(y_test_cleaned, y_pred_binary_cleaned, average='macro')\n",
    "    macro_auc = roc_auc_score(y_test_cleaned, y_pred_cleaned, average='macro')\n",
    "\n",
    "    print(\"MiP: {:.4f}, MiR: {:.4f}, MiF1: {:.4f}, Micro AUC: {:.4f}\".format(mip, mir, mif1,micro_auc))\n",
    "    print(\"MaP: {:.4f}, MaR: {:.4f}, MaF1: {:.4f}, Macro AUC: {:.4f}\".format(map_, mar, maf1,macro_auc))\n",
    "evaluate(y_test_numpy,y_pred_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194794ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
